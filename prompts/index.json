{
  "generatedAt": "2026-01-28T13:56:16.703Z",
  "items": [
    {
      "id": "ai-prd-generator",
      "title": "AI-Ready PRD Generator",
      "category": "discovery",
      "path": "discovery/ai-prd-generator",
      "tags": [
        "prd",
        "ai",
        "requirements",
        "specification"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Senior Product Manager who writes PRDs tailored for AI coding agents (Cursor, Claude Code).\n\n# Context\n\nWe are defining **{{product_name}}**, a {{product_type}} for **{{target_users}}**.\nProblem: **{{problem_statement}}**.\nKey workflows: **{{key_workflows}}**.\nAI role: **{{ai_role}}**.\nSuccess metrics: **{{success_metrics}}**.\nConstraints: **{{constraints}}**.\nExisting stack: **{{existing_stack}}**.\n\n# Task\n\nCreate a PRD that an AI engineer can use as the primary build reference.\n\n# Requirements\n\nInclude the following sections:\n\n1. **Summary**\n   - One-paragraph overview of the product and why it matters.\n2. **Goals & Non-Goals**\n   - 3–5 measurable goals and explicit out-of-scope items.\n3. **Target Users & Jobs-to-be-Done**\n   - Personas, JTBD, and success criteria for each.\n4. **User Journeys**\n   - 2–3 step-by-step flows that match the key workflows.\n5. **Functional Requirements**\n   - Numbered list with acceptance criteria and priority tags (Must/Should/Could).\n6. **AI Interaction Design**\n   - Prompt strategy, system constraints, tone, and when to ask for confirmation.\n7. **Data & Integrations**\n   - Inputs the AI needs, data sources, APIs, and required permissions.\n8. **Non-Functional Requirements**\n   - Performance, reliability, security, privacy, compliance.\n9. **Metrics & Instrumentation**\n   - Events to track, dashboards, and success thresholds.\n10. **Risks & Open Questions**\n    - Unknowns that could block delivery and how to validate them.\n\n# Output Format\n\nReturn the entire PRD as a **single markdown code block** so it can be easily copied and pasted. Use clear headings, tables where helpful, and concise bullets within the code block.\n",
      "slug": "ai-prd-generator",
      "shortDescription": "Create a PRD optimized for AI coding agents in Cursor or Claude Code.",
      "surface": "discovery",
      "pmStage": "Requirements & Specifications",
      "featured": true,
      "complexity": "Structured",
      "outputType": "Markdown PRD",
      "jobsToBeDone": [
        "Translate ideas into AI-ready requirements",
        "Give AI coding tools a clear build spec",
        "Align stakeholders on scope and constraints"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product or Feature Name",
          "example": "SupportOps Assistant"
        },
        {
          "key": "product_type",
          "label": "Product Type",
          "example": "Internal AI assistant for customer support"
        },
        {
          "key": "target_users",
          "label": "Target Users",
          "example": "Tier-1 support agents and team leads"
        },
        {
          "key": "problem_statement",
          "label": "Problem Statement",
          "example": "Agents spend too long summarizing tickets and locating policy answers"
        },
        {
          "key": "key_workflows",
          "label": "Key Workflows",
          "example": "Summarize ticket, draft response, cite policy, log disposition"
        },
        {
          "key": "ai_role",
          "label": "AI Role / Behavior",
          "example": "Acts as a co-pilot that proposes drafts but requires human approval"
        },
        {
          "key": "success_metrics",
          "label": "Success Metrics",
          "example": "Reduce average handle time by 25%; 90% of drafts accepted"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "No PII leaves VPC; must integrate with Zendesk"
        },
        {
          "key": "existing_stack",
          "label": "Existing Stack",
          "example": "React, Node.js, Postgres, AWS"
        }
      ]
    },
    {
      "id": "ai-risk-assessment",
      "title": "AI/ML Risk Assessment Framework",
      "category": "discovery",
      "path": "discovery/ai-risk-assessment",
      "tags": [
        "ai",
        "ml",
        "risk",
        "bias",
        "fairness",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are an AI Risk Manager and ML Ethics specialist conducting a comprehensive risk assessment for an AI feature launch.\n\n# Context\nWe are preparing to launch **{{ai_feature_name}}**.\nModel type: **{{model_type}}**.\nTraining data: **{{training_data_source}}**.\nDecision impact: **{{decision_impact}}**.\nUser population: **{{user_population}}**.\nRegulatory context: **{{regulatory_context}}**.\n\n# Task\nCreate a comprehensive AI/ML risk assessment covering all dimensions of AI risk, with concrete mitigation strategies and monitoring plans.\n\n# Requirements\n\n## 1. Model Performance Risks\n\n### Accuracy & Reliability\n- **False Positives**: What happens when AI incorrectly flags something?\n- **False Negatives**: What happens when AI misses something important?\n- **Confidence Calibration**: Is the model's confidence score reliable?\n- **Edge Cases**: What inputs might cause unexpected behavior?\n\n### Model Degradation\n- **Data Drift**: How will distribution changes affect performance?\n- **Concept Drift**: How will real-world changes affect model assumptions?\n- **Staleness**: How quickly does the model become outdated?\n\n## 2. Bias & Fairness Risks\n\n### Training Data Bias\n- What biases might exist in the training data?\n- Which groups might be underrepresented?\n- How might historical biases be encoded?\n\n### Algorithmic Bias\n- Could the model treat different groups unfairly?\n- What protected attributes might correlate with features?\n- How could bias manifest in outcomes?\n\n### Measurement Bias\n- How was ground truth determined in training data?\n- Could labeling have introduced bias?\n\n## 3. Explainability & Transparency Risks\n\n### Interpretability\n- Can we explain why the model made a decision?\n- What level of explanation do users/regulators need?\n- Are there \"black box\" components?\n\n### Auditability\n- Can decisions be traced and reviewed?\n- Is there sufficient logging for compliance?\n- Can we reproduce past decisions?\n\n## 4. Security & Adversarial Risks\n\n### Adversarial Attacks\n- How could bad actors manipulate inputs?\n- What prompt injection risks exist (for LLMs)?\n- Could the model be fooled by edge cases?\n\n### Data Poisoning\n- Could training data be manipulated?\n- How do we validate data integrity?\n\n### Model Extraction\n- Could competitors reverse-engineer the model?\n- What IP protection is needed?\n\n## 5. Operational Risks\n\n### Availability & Latency\n- What if the model service goes down?\n- What latency is acceptable for the use case?\n- What's the fallback if AI is unavailable?\n\n### Scalability\n- Can the model handle peak loads?\n- What are the cost implications of scale?\n\n### Versioning & Rollback\n- How do we safely update models?\n- Can we quickly rollback if issues arise?\n\n## 6. Ethical & Societal Risks\n\n### Automation Impact\n- Does this displace human workers?\n- Does it augment or replace human judgment?\n- Are there skills atrophy concerns?\n\n### Misuse Potential\n- Could this feature be misused?\n- What guardrails prevent harmful use?\n\n### Transparency to End Users\n- Do users know they're interacting with AI?\n- Do they understand its limitations?\n\n## 7. Regulatory & Compliance Risks\n\n### Current Regulations\n- GDPR implications (automated decision-making)\n- Industry-specific regulations\n- AI-specific regulations (EU AI Act)\n\n### Documentation Requirements\n- Model cards and documentation needs\n- Bias audits and fairness reports\n- Human oversight requirements\n\n## Risk Register Format\n\nFor each identified risk, provide:\n- **Risk ID**: Unique identifier\n- **Category**: Performance/Bias/Explainability/Security/Operational/Ethical/Regulatory\n- **Description**: Clear description of the risk\n- **Likelihood**: Low/Medium/High\n- **Impact**: Low/Medium/High (if risk materializes)\n- **Risk Score**: Likelihood × Impact\n- **Mitigation Strategy**: Preventive actions\n- **Detection Method**: How we'll know if it happens\n- **Response Plan**: What to do if it occurs\n- **Owner**: Who is responsible\n- **Monitoring Metrics**: KPIs to track\n\n## Output\n1. Executive summary with top 5 risks\n2. Detailed risk register table\n3. Mitigation roadmap with priorities\n4. Monitoring dashboard requirements\n5. Escalation procedures\n\n# Output Format\nStructured risk assessment with tables, clear categories, and actionable mitigations.\n",
      "slug": "ai-risk-assessment",
      "shortDescription": "Identify and mitigate AI-specific risks including bias, drift, hallucination, and explainability.",
      "surface": "discovery",
      "pmStage": "Strategy & Decision Artifacts",
      "teamScale": "Enterprise",
      "complexity": "Deep",
      "outputType": "Risk Register",
      "jobsToBeDone": [
        "Identify AI-specific risks before launch",
        "Plan mitigation strategies for ML failures",
        "Satisfy enterprise compliance requirements"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Intelligent Contract Analysis"
        },
        {
          "key": "model_type",
          "label": "Model Type",
          "example": "LLM-based document extraction with fine-tuned NER"
        },
        {
          "key": "training_data_source",
          "label": "Training Data Source",
          "example": "100K historical contracts from 50 enterprise customers"
        },
        {
          "key": "decision_impact",
          "label": "Decision Impact",
          "example": "Automated clause flagging that triggers legal review workflows"
        },
        {
          "key": "user_population",
          "label": "User Population",
          "example": "Legal teams at Fortune 500 companies across US, EU, APAC"
        },
        {
          "key": "regulatory_context",
          "label": "Regulatory Context",
          "example": "GDPR, SOC 2, potential AI Act implications"
        }
      ]
    },
    {
      "id": "ai-user-research",
      "title": "AI Feature User Research Framework",
      "category": "discovery",
      "path": "discovery/ai-user-research",
      "tags": [
        "ai",
        "user-research",
        "trust",
        "automation",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are a UX Researcher specializing in AI/ML product experiences and human-AI interaction design.\n\n# Context\nWe are developing **{{ai_feature_name}}** for **{{target_users}}**.\nCurrent workflow: **{{current_workflow}}**.\nAI capability: **{{ai_capability}}**.\nError consequences: **{{error_consequence}}**.\n\n# Task\nCreate a comprehensive user research framework to understand how users will interact with, trust, and adopt this AI feature.\n\n# Requirements\n\n## 1. User Segmentation by AI Readiness\nIdentify 3-4 user segments based on:\n- **AI Literacy**: Technical understanding of AI/ML\n- **Trust Disposition**: Skeptic, Cautious, or Early Adopter\n- **Risk Tolerance**: How they handle AI errors\n- **Autonomy Preference**: Full automation vs. human-in-the-loop\n\n## 2. Key Research Questions\nFor each segment, define questions to understand:\n- Current pain points in manual workflow\n- Mental models about AI capabilities and limitations\n- Trust calibration (when do they trust vs. verify?)\n- Error recovery expectations\n- Control and override needs\n- Transparency and explainability requirements\n\n## 3. AI-Specific User Journey Mapping\nMap the journey including:\n- **First encounter**: How do users discover the AI feature?\n- **Trust building**: What evidence do they need?\n- **Adoption curve**: Progressive trust building stages\n- **Error experience**: How they discover and handle AI mistakes\n- **Feedback loops**: How they correct and train the AI\n\n## 4. Human-AI Interaction Patterns\nRecommend interaction patterns for each segment:\n- **Full automation** - AI acts autonomously\n- **AI-assisted** - AI suggests, human approves\n- **AI-on-demand** - Human triggers AI when needed\n- **Human-first** - Human does work, AI validates\n\n## 5. Trust Calibration Signals\nIdentify what users need to see:\n- Confidence indicators\n- Explanation of AI decisions\n- Audit trails and provenance\n- Comparison to manual results\n- Performance metrics over time\n\n## 6. Interview Guide\nProvide 10 key interview questions specifically for AI feature research.\n\n## 7. Success Metrics for User Adoption\nDefine metrics to track:\n- Adoption rate by segment\n- Trust calibration accuracy (do users appropriately trust/distrust?)\n- Override rate (healthy skepticism vs. lack of trust)\n- Time to full automation confidence\n- User satisfaction with AI assistance\n\n# Output Format\nStructured research framework with clear sections, tables for segments, and actionable interview questions.\n",
      "slug": "ai-user-research",
      "shortDescription": "Understand user needs, trust levels, and AI literacy for intelligent features.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "teamScale": "Enterprise",
      "complexity": "Structured",
      "outputType": "Research Framework",
      "jobsToBeDone": [
        "Understand user readiness for AI features",
        "Identify trust barriers and enablers",
        "Design appropriate human-AI interaction patterns"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Intelligent Document Processing"
        },
        {
          "key": "target_users",
          "label": "Target Users",
          "example": "Enterprise finance teams processing invoices"
        },
        {
          "key": "current_workflow",
          "label": "Current Manual Workflow",
          "example": "Manual data entry from PDFs into ERP, 15 mins per invoice"
        },
        {
          "key": "ai_capability",
          "label": "AI Capability Being Introduced",
          "example": "Automated extraction and validation of invoice fields"
        },
        {
          "key": "error_consequence",
          "label": "Consequence of AI Errors",
          "example": "Incorrect payments, audit failures, vendor disputes"
        }
      ]
    },
    {
      "id": "competitor-comparison",
      "title": "Competitor Comparison & Differentiation",
      "category": "discovery",
      "path": "discovery/competitor-comparison",
      "tags": [
        "competition",
        "market-analysis",
        "differentiation",
        "positioning"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a product strategy lead specializing in competitive intelligence and positioning.\n\n# Context\nWe are assessing the {{our_product_category}} landscape to position **{{our_product}}** against **{{competitor_a}}** and **{{competitor_b}}**.\nOur primary target is **{{target_segment}}** and our differentiation goal is: **{{differentiation_goal}}**.\n\n# Task\nCreate a competitive comparison that identifies where we can win, lose, and differentiate.\n\n# Requirements\n1. **Comparison Matrix**\n   - Evaluate both competitors across: Target ICP, Core use cases, Strengths, Weaknesses, UX/Workflow philosophy, Pricing & packaging, GTM motion, and Moat.\n2. **Strategic Insights**\n   - Call out the top 3 gaps or unmet needs in the market.\n   - Identify the most likely reasons prospects choose each competitor.\n3. **Differentiation Opportunities**\n   - Propose 2-3 concrete wedges for {{our_product}} tied to {{target_segment}}.\n   - Include messaging angles and product/packaging implications.\n4. **Risks & Unknowns**\n   - List key assumptions and what research is needed to validate them.\n\n# Output Format\n1. **Executive Summary** (3 bullets)\n2. **Comparison Matrix** (table)\n3. **Strategic Insights** (bullets)\n4. **Differentiation Opportunities** (bullets with recommended actions)\n5. **Risks & Open Questions** (bullets)\n\nUse clear, neutral language and avoid unsupported claims.\n",
      "slug": "competitor-comparison",
      "shortDescription": "Compare competitors and identify concrete differentiation opportunities.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "complexity": "Structured",
      "outputType": "Comparison Matrix + Strategy",
      "jobsToBeDone": [
        "Benchmark against competitors",
        "Identify a defensible wedge",
        "Inform positioning and roadmap"
      ],
      "inputs": [
        {
          "key": "our_product",
          "label": "Our Product",
          "example": "RocketChat"
        },
        {
          "key": "our_product_category",
          "label": "Product Category",
          "example": "Team communication & collaboration"
        },
        {
          "key": "competitor_a",
          "label": "Competitor A",
          "example": "Slack"
        },
        {
          "key": "competitor_b",
          "label": "Competitor B",
          "example": "Microsoft Teams"
        },
        {
          "key": "target_segment",
          "label": "Target Segment",
          "example": "Highly regulated enterprises (healthcare, government)"
        },
        {
          "key": "differentiation_goal",
          "label": "Differentiation Goal",
          "example": "Win deals where data sovereignty and customization are critical"
        }
      ]
    },
    {
      "id": "feature-prioritization",
      "title": "Feature Prioritization Framework",
      "category": "discovery",
      "path": "discovery/feature-prioritization",
      "tags": [
        "prioritization",
        "roadmap",
        "RICE",
        "MoSCoW"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Senior Product Manager skilled in backlog prioritization and outcome-driven planning.\n\n# Context\nWe are prioritizing the {{product_name}} roadmap to achieve: **{{objective}}**.\nConstraints: **{{constraints}}**.\n\n# Task\nPrioritize the following features using **{{framework_preference}}** (or pick the best-fitting framework if not specified):\n{{feature_list}}\n\n# Requirements\n1. **Framework Selection**\n   - Name the framework and why it fits the objective and constraints.\n2. **Scoring Table**\n   - Score each feature using the framework’s criteria.\n3. **Ranked List**\n   - Provide the final ranking with a 1–2 sentence rationale per feature.\n4. **Quick-Win vs. Big-Bet**\n   - Identify at least one quick-win and one longer-term bet.\n5. **Risks & Dependencies**\n   - Note any major dependencies or delivery risks.\n\n# Output Format\n1. **Framework Choice** (short paragraph)\n2. **Scoring Table** (markdown table)\n3. **Ranked Recommendations** (numbered list)\n4. **Quick-Win & Big-Bet** (bullets)\n5. **Risks & Dependencies** (bullets)\n",
      "slug": "feature-prioritization",
      "shortDescription": "Rank features using a structured scoring framework and clear rationale.",
      "surface": "discovery",
      "pmStage": "Strategy & Decision Artifacts",
      "complexity": "Structured",
      "outputType": "Prioritized List + Scoring",
      "jobsToBeDone": [
        "Rank a feature backlog",
        "Explain tradeoffs to stakeholders",
        "Align priorities to goals"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "TaskFlow"
        },
        {
          "key": "objective",
          "label": "Primary Objective",
          "example": "Increase activation from 32% to 45%"
        },
        {
          "key": "feature_list",
          "label": "Feature List",
          "example": "Dark mode, Offline support, Social login, Export to PDF"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "2 engineers for 6 weeks; must ship 1 quick-win"
        },
        {
          "key": "framework_preference",
          "label": "Framework Preference",
          "example": "RICE (if unknown, choose best fit)"
        }
      ]
    },
    {
      "id": "feedback-summary",
      "title": "Customer Feedback Synthesis",
      "category": "discovery",
      "path": "discovery/feedback-summary",
      "tags": [
        "feedback",
        "user-research",
        "pain-points",
        "voice-of-customer"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Senior Product Manager skilled in qualitative data analysis and voice-of-customer research.\n\n# Context\nWe have collected feedback for **{{product_name}}** from {{feedback_sources}} over **{{time_window}}**.\nThe segment of interest is **{{target_segment}}**.\n\n# Task\nAnalyze the feedback below and synthesize actionable insights.\n\n\"{{feedback_text}}\"\n\n# Requirements\n1. **Sentiment Overview**\n   - Overall sentiment (Positive/Neutral/Negative) with a brief rationale.\n2. **Theme Breakdown**\n   - Group feedback into 3–5 themes and estimate relative frequency (e.g., High/Medium/Low).\n3. **Critical Issues**\n   - Identify blockers or bugs and tag severity as **P0/P1/P2** with rationale.\n4. **Feature Requests**\n   - Separate into **Must-Have** (table stakes) vs **Nice-to-Have** (delighters).\n5. **Voice of Customer**\n   - Provide a representative quote per theme.\n6. **Recommended Next Steps**\n   - 3 concrete actions: quick fix, research follow-up, and roadmap candidate.\n\n# Output Format\n1. **Executive Summary** (3 bullets max)\n2. **Theme Table** (Theme | Frequency | Example Quote)\n3. **Critical Issues** (bullets)\n4. **Feature Requests** (Must-Have / Nice-to-Have)\n5. **Recommended Next Steps** (bullets)\n",
      "slug": "feedback-summary",
      "shortDescription": "Distill raw feedback into themes, severity, and actionable next steps.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "complexity": "Quick",
      "outputType": "Insight Report",
      "jobsToBeDone": [
        "Synthesize raw feedback",
        "Identify urgent issues",
        "Prioritize feature requests"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "PhotoEditor Pro"
        },
        {
          "key": "feedback_sources",
          "label": "Feedback Sources",
          "example": "Support tickets, App Store reviews, NPS survey"
        },
        {
          "key": "time_window",
          "label": "Time Window",
          "example": "Last 60 days"
        },
        {
          "key": "target_segment",
          "label": "Target Segment",
          "example": "Pro users on annual plans"
        },
        {
          "key": "feedback_text",
          "label": "Feedback Text",
          "example": "App crashes on export. Great filters but slow. Subscription is too expensive."
        }
      ]
    },
    {
      "id": "legal-ip-review",
      "title": "Legal & IP Review Checklist",
      "category": "discovery",
      "path": "discovery/legal-ip-review",
      "tags": [
        "legal",
        "ip",
        "copyright",
        "licensing",
        "compliance",
        "open-source"
      ],
      "updatedAt": "2026-01-28T13:56:11.605Z",
      "prompt": "# Role\n\nYou are a Legal & IP Advisor helping product teams identify intellectual property, licensing, and compliance considerations early in the development process.\n\n# Context\n\nWe are building **{{product_name}}**.\n\n**Product Description**: {{product_description}}\n\n**Tech Stack & Dependencies**: {{tech_stack}}\n\n**Data Sources**: {{data_sources}}\n\n**Third-Party Services/APIs**: {{third_party_services}}\n\n{{#if target_market}}\n**Target Market**: {{target_market}}\n{{/if}}\n\n{{#if similar_products}}\n**Similar Products**: {{similar_products}}\n{{/if}}\n\n# Task\n\nCreate a comprehensive legal and IP review checklist that identifies potential issues and action items before development proceeds.\n\n# Requirements\n\n## 1. Open Source License Compliance\n\nReview the tech stack and flag:\n- **License Types**: Categorize dependencies by license (MIT, Apache 2.0, GPL, LGPL, etc.)\n- **Copyleft Risks**: Flag any GPL/AGPL that could affect distribution\n- **Attribution Requirements**: List required notices and attributions\n- **Commercial Use Restrictions**: Any licenses limiting commercial use\n\n## 2. Third-Party API & Service Review\n\nFor each third-party service:\n- **Terms of Service**: Key restrictions (rate limits, usage caps, data retention)\n- **Data Processing**: Where data is processed, data residency requirements\n- **Output Ownership**: Who owns AI-generated content (critical for LLM APIs)\n- **Liability & Indemnification**: Key risk areas\n- **Exit Strategy**: Data portability, vendor lock-in concerns\n\n## 3. Data & Content Rights\n\nEvaluate data sources:\n- **Ownership**: Do we have rights to use this data?\n- **Privacy**: PII, GDPR, CCPA considerations\n- **Training Data**: Rights for ML training (if applicable)\n- **User-Generated Content**: Terms for content created by/for users\n- **Scraping/Collection**: How was data obtained?\n\n## 4. Intellectual Property Considerations\n\n### Copyright\n- Content originality requirements\n- Fair use considerations\n- DMCA compliance needs\n\n### Trademarks\n- Product name availability\n- Similar product name conflicts\n- Domain availability\n\n### Patents\n- Freedom to operate concerns\n- Prior art for novel features\n- Defensive considerations\n\n### Trade Secrets\n- Confidential information handling\n- NDA requirements with vendors/partners\n\n## 5. AI-Specific Considerations (if applicable)\n\n- **Model Licensing**: Terms for using base models\n- **Output Rights**: Ownership of AI-generated content\n- **Disclosure Requirements**: When must AI use be disclosed?\n- **Liability for Outputs**: Who is liable for AI errors/hallucinations?\n- **Training Data Provenance**: Legal status of training data\n\n## 6. Compliance Checklist by Region\n\nFlag relevant regulations:\n- **US**: CCPA, CAN-SPAM, COPPA, Section 230\n- **EU**: GDPR, AI Act, Digital Services Act\n- **Industry-Specific**: HIPAA, PCI-DSS, SOC 2, etc.\n\n## 7. Action Items\n\nPrioritize into:\n- **Blockers**: Must resolve before launch\n- **Pre-Launch**: Should resolve before GA\n- **Post-Launch**: Can address after initial release\n- **Legal Review Needed**: Requires actual legal counsel\n\n# Output Format\n\nReturn the entire review as a **single markdown code block** so it can be easily copied and pasted. Use checklists, tables, and clear categorization. Include a summary of top risks at the beginning.\n",
      "slug": "legal-ip-review",
      "shortDescription": "Review copyright, licensing, and IP considerations before building.",
      "surface": "discovery",
      "pmStage": "Risk, Quality & Validation",
      "complexity": "Structured",
      "outputType": "Legal Checklist",
      "jobsToBeDone": [
        "Identify IP and licensing risks early",
        "Review open-source compliance",
        "Flag copyright and trademark issues",
        "Create action items for legal review"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product/Feature Name",
          "example": "AI Support Chatbot"
        },
        {
          "key": "product_description",
          "label": "Product Description",
          "example": "A chatbot that uses LLM to answer customer support questions based on our FAQ database"
        },
        {
          "key": "tech_stack",
          "label": "Tech Stack & Dependencies",
          "example": "Next.js (MIT), OpenAI API, shadcn/ui (MIT), Tailwind CSS (MIT)"
        },
        {
          "key": "data_sources",
          "label": "Data Sources",
          "example": "Internal FAQ database, customer support tickets (anonymized), public product documentation"
        },
        {
          "key": "third_party_services",
          "label": "Third-Party Services/APIs",
          "example": "OpenAI API, Vercel hosting, Sentry error tracking"
        },
        {
          "key": "target_market",
          "label": "Target Market/Region",
          "example": "US and EU enterprise customers",
          "optional": true
        },
        {
          "key": "similar_products",
          "label": "Similar Products (for trademark check)",
          "example": "Intercom, Zendesk Answer Bot, Drift",
          "optional": true
        }
      ]
    },
    {
      "id": "market-analysis",
      "title": "Market Analysis Brief",
      "category": "discovery",
      "path": "discovery/market-analysis",
      "tags": [
        "market-research",
        "competitors",
        "swot",
        "tam-sam-som"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Senior Product Strategist with expertise in market research and competitive analysis.\n\n# Context\nWe are evaluating the **{{industry}}** market for **{{customer_segment}}** in **{{geography}}**.\nBusiness model: **{{business_model}}**. Time horizon: **{{time_horizon}}**.\n\n# Task\nProvide a concise market analysis to inform product and go-to-market decisions.\n\n# Requirements\n1. **Market Size & Growth**\n   - Provide TAM/SAM/SOM estimates with assumptions and confidence level.\n2. **Demand Drivers & Trends**\n   - 3–5 trends shaping buying behavior in the next {{time_horizon}}.\n3. **Competitive Landscape**\n   - Top 3–5 competitors and how they segment the market.\n4. **Pricing & Packaging Norms**\n   - Typical price bands or value metrics used in this market.\n5. **Opportunities & Risks**\n   - 2–3 opportunities and 2–3 risks specific to this segment.\n\n# Output Format\n1. **Executive Summary** (3 bullets)\n2. **Market Size & Growth** (bullets + assumptions)\n3. **Key Trends** (bullets)\n4. **Competitive Landscape** (table)\n5. **Opportunities & Risks** (bullets)\n",
      "slug": "market-analysis",
      "shortDescription": "Assess market size, trends, and competitive dynamics for a target segment.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "complexity": "Structured",
      "outputType": "Market Brief",
      "jobsToBeDone": [
        "Understand market trends",
        "Estimate market potential",
        "Map competitive landscape"
      ],
      "inputs": [
        {
          "key": "industry",
          "label": "Industry or Niche",
          "example": "SaaS Project Management"
        },
        {
          "key": "geography",
          "label": "Target Geography",
          "example": "North America + UK"
        },
        {
          "key": "customer_segment",
          "label": "Customer Segment",
          "example": "Mid-market tech companies (200-2,000 employees)"
        },
        {
          "key": "business_model",
          "label": "Business Model",
          "example": "B2B SaaS"
        },
        {
          "key": "time_horizon",
          "label": "Time Horizon",
          "example": "Next 24 months"
        }
      ]
    },
    {
      "id": "marketing-ideation",
      "title": "Marketing Message Ideation",
      "category": "discovery",
      "path": "discovery/marketing-ideation",
      "tags": [
        "marketing",
        "copywriting",
        "launch",
        "positioning"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a creative copywriter who turns product benefits into compelling, audience-specific messaging.\n\n# Context\nWe are launching **{{product_description}}** for **{{target_audience}}**.\nPrimary benefit: **{{primary_benefit}}**. Differentiator: **{{differentiator}}**.\nBrand voice: **{{brand_voice}}**. Channel: **{{channel}}**.\n\n# Task\nGenerate messaging options that communicate value quickly and memorably.\n\n# Requirements\n1. **Value Proposition Statements** (3 options)\n   - 1–2 sentences each, outcome-focused.\n2. **Taglines** (5 options)\n   - Short, memorable, channel-appropriate.\n3. **CTA Suggestions** (3 options)\n   - Actionable and aligned with the audience.\n4. **Rationale**\n   - Briefly explain why each option should resonate.\n\n# Output Format\n1. **Value Proposition Statements** (numbered list)\n2. **Taglines** (bullets)\n3. **CTA Suggestions** (bullets)\n4. **Rationale** (short notes under each section)\n",
      "slug": "marketing-ideation",
      "shortDescription": "Generate value props and taglines aligned to a specific audience and brand voice.",
      "surface": "discovery",
      "pmStage": "Messaging & Content",
      "complexity": "Quick",
      "outputType": "Messaging Options",
      "jobsToBeDone": [
        "Create marketing copy",
        "Define value propositions",
        "Test messaging angles"
      ],
      "inputs": [
        {
          "key": "product_description",
          "label": "Product Description",
          "example": "An AI-powered meal planner that reduces food waste"
        },
        {
          "key": "primary_benefit",
          "label": "Primary Benefit",
          "example": "Save money by using ingredients you already have"
        },
        {
          "key": "target_audience",
          "label": "Target Audience",
          "example": "Eco-conscious millennials"
        },
        {
          "key": "differentiator",
          "label": "Key Differentiator",
          "example": "Generates meals from pantry inventory in under 30 seconds"
        },
        {
          "key": "brand_voice",
          "label": "Brand Voice",
          "example": "Optimistic, friendly, practical"
        },
        {
          "key": "channel",
          "label": "Primary Channel",
          "example": "Landing page hero + paid social ads"
        }
      ]
    },
    {
      "id": "prd-outline",
      "title": "PRD Outline Builder",
      "category": "discovery",
      "path": "discovery/prd-outline",
      "tags": [
        "prd",
        "requirements",
        "scoping",
        "planning"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Product Manager skilled in defining clear, comprehensive requirements.\n\n# Context\n\nWe are defining **{{product_name}}**, a {{product_type}} for **{{target_segment}}**.\nProblem: **{{problem_statement}}**. Business goal: **{{business_goal}}**.\nConstraints: **{{constraints}}**.\n\n# Task\n\nGenerate a Product Requirements Document (PRD) outline.\n\n# Requirements\n\nInclude the following sections:\n\n1. **Problem & Opportunity**\n   - Problem statement and why it matters now.\n2. **Goals & Success Metrics**\n   - Business and user goals, with primary KPIs.\n3. **Target Users & Jobs-to-be-Done**\n   - Personas, JTBD, and key use cases.\n4. **Scope**\n   - MVP feature list, non-goals, and assumptions.\n5. **User Stories**\n   - 4–6 core user stories tied to outcomes.\n6. **Requirements & Constraints**\n   - Functional requirements, platform considerations, and constraints.\n7. **Dependencies & Risks**\n   - Notable technical or business dependencies.\n8. **Open Questions**\n   - What remains to be validated.\n\n# Output Format\n\nReturn the entire PRD outline as a **single markdown code block** so it can be easily copied and pasted. Use clear headings and bullets within the code block.\n",
      "slug": "prd-outline",
      "shortDescription": "Create a structured PRD outline tailored to a product goal and audience.",
      "surface": "discovery",
      "pmStage": "Requirements & Specifications",
      "featured": true,
      "complexity": "Structured",
      "outputType": "Markdown Outline",
      "jobsToBeDone": [
        "Draft a PRD structure",
        "Clarify goals, scope, and success metrics",
        "Align stakeholders on requirements"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "PulseFit"
        },
        {
          "key": "product_type",
          "label": "Product Type",
          "example": "Mobile Fitness App"
        },
        {
          "key": "target_segment",
          "label": "Target User Segment",
          "example": "Busy professionals working from home"
        },
        {
          "key": "problem_statement",
          "label": "Problem Statement",
          "example": "Users struggle to fit workouts into short breaks between meetings"
        },
        {
          "key": "business_goal",
          "label": "Business Goal",
          "example": "Increase 7-day activation to 40%"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "MVP within 8 weeks; no wearables integration at launch"
        }
      ]
    },
    {
      "id": "pricing-strategy",
      "title": "Pricing Strategy Review",
      "category": "discovery",
      "path": "discovery/pricing-strategy",
      "tags": [
        "pricing",
        "monetization",
        "strategy",
        "packaging"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Marketing Manager with expertise in pricing, packaging, and monetization strategy.\n\n# Context\nWe are launching a **{{product_type}}** for **{{target_customer}}**.\nValue metric: **{{value_metric}}**. Benchmarks: **{{competitive_benchmarks}}**.\nBusiness goal: **{{business_goal}}**.\n\n# Task\nRecommend a pricing strategy and packaging approach.\n\n# Requirements\n1. **Model Comparison**\n   - Compare Freemium, Tiered Subscription, and Usage-based pricing for this context.\n2. **Packaging Recommendation**\n   - Propose 2–3 tiers with target customer fit and value metric alignment.\n3. **Price Point Guidance**\n   - Provide a directional price range and rationale vs. benchmarks.\n4. **Metrics to Monitor**\n   - CAC payback, conversion, expansion, churn, and ARPA.\n5. **Risks & Experiments**\n   - Key risks and a 60–90 day pricing experiment plan.\n\n# Output Format\n1. **Summary Recommendation** (short paragraph)\n2. **Model Comparison** (table)\n3. **Packaging Proposal** (table)\n4. **Metrics to Monitor** (bullets)\n5. **Risks & Experiments** (bullets)\n",
      "slug": "pricing-strategy",
      "shortDescription": "Compare pricing models and recommend packaging with key metrics to monitor.",
      "surface": "discovery",
      "pmStage": "Strategy & Decision Artifacts",
      "complexity": "Deep",
      "outputType": "Pricing Recommendation",
      "jobsToBeDone": [
        "Evaluate pricing models",
        "Recommend packaging and value metric",
        "Define pricing risks and experiments"
      ],
      "inputs": [
        {
          "key": "product_type",
          "label": "Product Type",
          "example": "B2B SaaS productivity tool"
        },
        {
          "key": "target_customer",
          "label": "Target Customer",
          "example": "Mid-market teams (100-500 seats)"
        },
        {
          "key": "value_metric",
          "label": "Primary Value Metric",
          "example": "Per active seat"
        },
        {
          "key": "competitive_benchmarks",
          "label": "Competitive Benchmarks",
          "example": "$12-25 per user/month; freemium common"
        },
        {
          "key": "business_goal",
          "label": "Business Goal",
          "example": "Reach $1M ARR in 12 months"
        }
      ]
    },
    {
      "id": "risk-analysis",
      "title": "Risk & Feasibility Assessment",
      "category": "discovery",
      "path": "discovery/risk-analysis",
      "tags": [
        "risk-management",
        "pre-mortem",
        "planning",
        "feasibility",
        "poc"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Risk Manager or Senior PM conducting a pre-mortem and feasibility assessment.\n\n# Context\n\nWe are assessing **{{feature_or_product}}** for {{assessment_stage}}.\nScope: **{{launch_scope}}**. Timeline: **{{timeline}}**.\nDependencies: **{{dependencies}}**.\n\n{{#if prd_summary}}\n**PRD Summary**: {{prd_summary}}\n{{/if}}\n\n{{#if tech_design_summary}}\n**Technical Design Summary**: {{tech_design_summary}}\n{{/if}}\n\n# Task\n\nIdentify and prioritize risks, assess feasibility, and propose concrete mitigations. Provide a clear go/no-go recommendation.\n\n# Requirements\n\n## 1. Feasibility Assessment\n\nEvaluate feasibility across these dimensions:\n- **Technical Feasibility**: Can we build this with available tech/skills?\n- **Resource Feasibility**: Do we have the time, budget, and people?\n- **Market Feasibility**: Is there validated demand? Any showstoppers?\n- **Operational Feasibility**: Can we support and maintain this?\n\nRate each dimension: **Green** (feasible), **Yellow** (concerns, but manageable), **Red** (significant blockers).\n\n## 2. Risk Register\n\nCategorize risks into:\n\n1. **Technical Risks** (complexity, unknowns, scalability, reliability, security)\n2. **Market Risks** (adoption, positioning, competition, timing)\n3. **Resource Risks** (skills gaps, capacity, dependencies on key people)\n4. **Operational Risks** (support, internal readiness, maintenance burden)\n5. **Legal/Compliance Risks** (privacy, regulatory, IP)\n\nFor each risk, include:\n\n- **Likelihood** (Low/Medium/High)\n- **Impact** (Low/Medium/High)\n- **Mitigation** (preventative and reactive)\n- **Early Warning Signal**\n- **Owner** (suggested function)\n\n## 3. Go/No-Go Recommendation\n\nProvide a clear recommendation:\n- **GO**: Proceed with POC/launch\n- **GO WITH CONDITIONS**: Proceed only if specific mitigations are in place\n- **NO-GO**: Do not proceed; explain blocking issues\n- **PIVOT**: Consider alternative approach\n\n# Output Format\n\nReturn the entire analysis as a **single markdown code block** so it can be easily copied and pasted. Include:\n1. Feasibility scorecard table\n2. Risk register table\n3. Top 3 focus risks with mitigation plans\n4. Go/No-Go recommendation with rationale\n",
      "slug": "risk-analysis",
      "shortDescription": "Assess feasibility and identify risks with go/no-go recommendation.",
      "surface": "discovery",
      "pmStage": "Risk, Quality & Validation",
      "complexity": "Structured",
      "outputType": "Risk Register",
      "jobsToBeDone": [
        "Assess technical and market feasibility",
        "Anticipate launch/POC risks",
        "Plan mitigation strategies",
        "Make go/no-go decisions"
      ],
      "inputs": [
        {
          "key": "feature_or_product",
          "label": "Feature/Product",
          "example": "AI-based Chatbot"
        },
        {
          "key": "assessment_stage",
          "label": "Assessment Stage",
          "example": "POC validation",
          "options": [
            "POC validation",
            "MVP launch",
            "Public beta",
            "Full launch"
          ]
        },
        {
          "key": "launch_scope",
          "label": "Scope",
          "example": "Internal POC with 5 pilot users"
        },
        {
          "key": "timeline",
          "label": "Timeline",
          "example": "3-5 days"
        },
        {
          "key": "dependencies",
          "label": "Key Dependencies",
          "example": "LLM vendor, design resources, test data"
        },
        {
          "key": "prd_summary",
          "label": "PRD Summary (optional)",
          "example": "Paste key sections from your PRD: goals, scope, requirements",
          "optional": true
        },
        {
          "key": "tech_design_summary",
          "label": "Tech Design Summary (optional)",
          "example": "Paste key sections: stack, architecture, key components",
          "optional": true
        }
      ]
    },
    {
      "id": "roadmap-planning",
      "title": "Roadmap Planning",
      "category": "discovery",
      "path": "discovery/roadmap-planning",
      "tags": [
        "roadmap",
        "planning",
        "strategy"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Product Manager focused on strategic planning and execution.\n\n# Context\n\nWe are planning the next 4 quarters for **{{product_name}}**.\nStrategic goals: **{{goals}}**. Constraints: **{{constraints}}**.\nKey metrics: **{{key_metrics}}**.\n\n# Task\n\nOutline a high-level product roadmap for the next 4 quarters.\n\n# Requirements\n\n- **Quarter Breakdown**: Q1–Q4 with 2–3 major initiatives each.\n- **Sequencing Logic**: Explain how each quarter builds toward the goals.\n- **Metrics Impact**: Tie each quarter to expected movement in key metrics.\n- **Dependencies**: Note major dependencies or cross-team needs.\n\n# Output Format\n\nReturn the entire roadmap as a **single markdown code block** so it can be easily copied and pasted. Include:\n\n1. **Roadmap Table** (Quarter | Theme | Initiatives | Metric Impact)\n2. **Sequencing Rationale** (short paragraph)\n3. **Dependencies & Risks** (bullets)\n",
      "slug": "roadmap-planning",
      "shortDescription": "Build a quarterly roadmap aligned to goals, capacity, and key metrics.",
      "surface": "discovery",
      "pmStage": "Strategy & Decision Artifacts",
      "complexity": "Deep",
      "outputType": "Roadmap Schedule",
      "jobsToBeDone": [
        "Plan quarterly milestones",
        "Align roadmap to goals",
        "Communicate sequencing logic"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "DevOps Dashboard"
        },
        {
          "key": "goals",
          "label": "Strategic Goals",
          "example": "Increase adoption, reduce churn, launch mobile app"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "Team of 5 engineers; SOC2 audit in Q4"
        },
        {
          "key": "key_metrics",
          "label": "Key Metrics",
          "example": "Activation rate, WAU, churn"
        }
      ]
    },
    {
      "id": "strategic-analysis",
      "title": "Strategic Market Analysis",
      "category": "discovery",
      "path": "discovery/strategic-analysis",
      "tags": [
        "strategy",
        "market-analysis",
        "discovery"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Chief Product Officer with deep expertise in market strategy and competitive analysis.\n\n# Context\n\nMarket domain: **{{market_domain}}**.\nCustomer segments: **{{customer_segments}}**.\nCompetitors: **{{competitors}}**.\nStrategic goal: **{{entry_goal}}**.\n\n# Task\n\nConduct a strategic analysis and recommend the best path forward.\n\n# Requirements\n\n1. **Market Overview**\n   - Key trends, growth drivers, and regulatory considerations.\n2. **Customer Needs**\n   - Core jobs-to-be-done and buying criteria by segment.\n3. **Competitive Landscape**\n   - Strengths/weaknesses and positioning of named competitors.\n4. **Strategic Options**\n   - 2–3 distinct options (e.g., niche vertical, platform play, partnership).\n5. **Recommendation**\n   - Choose one option and justify with risks, tradeoffs, and success metrics.\n\n# Output Format\n\nReturn the entire analysis as a **single markdown code block** so it can be easily copied and pasted. Include:\n\n1. **Executive Summary** (3 bullets)\n2. **Market & Customer Insights** (bullets)\n3. **Competitive Landscape** (table)\n4. **Strategic Options** (bullets)\n5. **Recommendation & Metrics** (bullets)\n",
      "slug": "strategic-analysis",
      "shortDescription": "Assess market dynamics and recommend a strategic entry or growth plan.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "complexity": "Deep",
      "outputType": "Strategic Report",
      "jobsToBeDone": [
        "Identify strategic opportunities",
        "Evaluate competitive threats",
        "Define a market entry or growth plan"
      ],
      "inputs": [
        {
          "key": "market_domain",
          "label": "Market / Domain",
          "example": "Generative AI for Legal Tech"
        },
        {
          "key": "customer_segments",
          "label": "Customer Segments",
          "example": "Mid-sized law firms, in-house legal teams"
        },
        {
          "key": "competitors",
          "label": "Main Competitors",
          "example": "LexisNexis, Casetext"
        },
        {
          "key": "entry_goal",
          "label": "Strategic Goal",
          "example": "Identify a defendable wedge for a new product"
        }
      ]
    },
    {
      "id": "tech-design-doc",
      "title": "Technical Design Doc Generator",
      "category": "discovery",
      "path": "discovery/tech-design-doc",
      "tags": [
        "architecture",
        "tech-stack",
        "design",
        "engineering"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Principal Software Architect who produces clear, pragmatic technical design documents.\n\n# Context\n\nApplication: **{{app_name}}**.\nDescription/PRD: **{{prd_or_description}}**.\nScale requirements: **{{scale_requirements}}**.\nNon-functional requirements: **{{non_functional_requirements}}**.\nConstraints: **{{constraints}}**.\nExisting stack: **{{existing_stack}}**.\n\n# Task\n\nCreate a technical design document that proposes the best architecture, stack, and implementation plan.\n\n# Requirements\n\nInclude the following sections:\n\n1. **Executive Summary**\n   - Design goals and how the architecture meets them.\n2. **Chosen Tech Stack**\n   - Frontend, backend, data, infra, and AI/ML (if applicable) with rationale.\n3. **System Architecture**\n   - High-level diagram (text) showing services and data flow.\n4. **Core Components**\n   - Responsibilities, dependencies, and scaling considerations.\n5. **Data Model**\n   - Key entities, storage choices, and retention strategy.\n6. **API & Integration Contracts**\n   - Main endpoints, events, or message topics.\n7. **Performance & Scalability**\n   - Caching, throughput assumptions, and bottlenecks.\n8. **Security & Compliance**\n   - Auth, authorization, data protection, and auditability.\n9. **Observability & Operations**\n   - Logging, metrics, alerts, and runbooks.\n10. **Phased Delivery Plan**\n\n- MVP → V1 milestones with risks and dependencies.\n\n# Output Format\n\nReturn the entire technical design document as a **single markdown code block** so it can be easily copied and pasted. Use clear headings, tables, and bullet points within the code block.\n",
      "slug": "tech-design-doc",
      "shortDescription": "Turn a PRD or high-level concept into an end-to-end technical design.",
      "surface": "discovery",
      "pmStage": "Code Templates & Schemas",
      "featured": true,
      "complexity": "Deep",
      "outputType": "Technical Design Document",
      "jobsToBeDone": [
        "Select a tech stack aligned to requirements",
        "Translate product scope into architecture",
        "Align engineering on implementation plan"
      ],
      "inputs": [
        {
          "key": "app_name",
          "label": "Application Name",
          "example": "FleetPulse"
        },
        {
          "key": "prd_or_description",
          "label": "PRD or App Description",
          "example": "A B2B dashboard for real-time fleet monitoring with alerts and route analytics"
        },
        {
          "key": "scale_requirements",
          "label": "Scale Requirements",
          "example": "5k concurrent users, 500k events/minute"
        },
        {
          "key": "non_functional_requirements",
          "label": "Non-Functional Requirements",
          "example": "99.9% uptime, SOC2 compliance, <200ms alert latency"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "Must deploy on AWS; prefer managed services"
        },
        {
          "key": "existing_stack",
          "label": "Existing Stack (if any)",
          "example": "React, Node.js, Postgres"
        }
      ]
    },
    {
      "id": "user-persona",
      "title": "User Persona Builder",
      "category": "discovery",
      "path": "discovery/user-persona",
      "tags": [
        "user-research",
        "persona",
        "empathy",
        "jtbd"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a UX researcher who creates empathetic, evidence-based personas.\n\n# Context\nWe are designing **{{product_name}}** for **{{target_user_description}}**.\nPrimary job-to-be-done: **{{primary_job}}**.\nUsage context: **{{context}}**.\n\n# Task\nCreate a comprehensive user persona to guide product decisions.\n\n# Requirements\nInclude:\n1. **Profile**: Name, age range, location, role, company size (if relevant).\n2. **Goals & Motivations**: What success looks like for them.\n3. **Pain Points**: Friction in their current workflow.\n4. **Behaviors & Tools**: How they work today and tools they use.\n5. **Decision Criteria**: What makes them adopt or reject a product.\n6. **Product Fit**: How {{product_name}} helps them achieve the job.\n7. **Quote**: A realistic quote capturing their mindset.\n\n# Output Format\nStructured persona profile with headings and bullets.\n",
      "slug": "user-persona",
      "shortDescription": "Create a detailed persona grounded in goals, behaviors, and buying criteria.",
      "surface": "discovery",
      "pmStage": "Research & Insight Reports",
      "featured": true,
      "complexity": "Quick",
      "outputType": "Persona Profile",
      "jobsToBeDone": [
        "Visualize target user",
        "Understand motivations and pain points",
        "Align team on user needs"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "TaskMaster"
        },
        {
          "key": "target_user_description",
          "label": "Target User Description",
          "example": "Freelance graphic designers managing multiple clients"
        },
        {
          "key": "primary_job",
          "label": "Primary Job-to-be-Done",
          "example": "Deliver client work on time without admin overload"
        },
        {
          "key": "context",
          "label": "Usage Context",
          "example": "Works from coworking spaces, mobile-first during client meetings"
        }
      ]
    },
    {
      "id": "ab-test",
      "title": "A/B Test Conversion Significance",
      "category": "execution",
      "path": "execution/ab-test",
      "tags": [
        "python",
        "statistics",
        "ab-testing"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Data Scientist supporting product experimentation decisions.\n\n# Context\nWe need to determine whether a new variant materially improved conversion relative to the control.\n\n# Task\nUsing the inputs below, calculate conversion rates, absolute/relative uplift, and test statistical significance.\n\n# Inputs\n- **Control data**: {{group_a_data}}\n- **Variant data**: {{group_b_data}}\n- **Significance threshold (alpha)**: {{alpha}}\n\n# Requirements\n1. **Test selection**: Use a two-proportion z-test when counts are large enough; otherwise fall back to Fisher's Exact Test.\n2. **Effect size**: Report absolute lift (pp) and relative lift (%).\n3. **Interpretation**: State whether the result is significant at the given alpha and what that means for a PM decision.\n4. **Reproducibility**: Print intermediate values (conversion rates, sample sizes, test used).\n\n# Output Format\n- Python code\n- Printed results with a short interpretation block\n",
      "slug": "ab-test",
      "shortDescription": "Quantify conversion uplift and statistical significance for a two-variant test.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Deep",
      "outputType": "Executed Code (Statistics)",
      "jobsToBeDone": [
        "Decide if an experiment shipped meaningful lift",
        "Report statistical confidence and effect size"
      ],
      "inputs": [
        {
          "key": "group_a_data",
          "label": "Control Group (conversions/visitors)",
          "example": "200 conversions out of 2000 users"
        },
        {
          "key": "group_b_data",
          "label": "Variant Group (conversions/visitors)",
          "example": "250 conversions out of 2100 users"
        },
        {
          "key": "alpha",
          "label": "Significance Threshold (alpha)",
          "example": "0.05"
        }
      ]
    },
    {
      "id": "ai-rollout-strategy",
      "title": "AI Feature Rollout Strategy",
      "category": "execution",
      "path": "execution/ai-rollout-strategy",
      "tags": [
        "ai",
        "ml",
        "rollout",
        "launch",
        "risk",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are an AI Product Launch specialist with expertise in safely rolling out ML-powered features, managing the transition from human to AI-assisted workflows.\n\n# Context\nWe are planning the rollout for **{{ai_feature_name}}**.\nCurrent process: **{{current_process}}**.\nUser segments: **{{user_segments}}**.\nRisk tolerance: **{{risk_tolerance}}**.\nSuccess metrics: **{{success_metrics}}**.\nTimeline constraints: **{{timeline_constraints}}**.\n\n# Task\nCreate a comprehensive rollout strategy that safely transitions users from human-only to AI-assisted workflows, with appropriate safeguards, feedback loops, and rollback procedures.\n\n# Requirements\n\n## 1. Rollout Philosophy\n\n### Progressive Trust Building\n- **Shadow Mode**: AI runs silently, predictions logged but not shown\n- **Suggestion Mode**: AI suggests, human decides\n- **Confirmation Mode**: AI acts, human confirms\n- **Autonomous Mode**: AI acts, human reviews exceptions\n\n### Confidence-Based Progression\n- Low confidence → Always human review\n- Medium confidence → User can accept/override\n- High confidence → Auto-process with audit trail\n\n## 2. Pre-Launch Validation\n\n### Model Readiness Checklist\n- [ ] Accuracy meets minimum thresholds on test set\n- [ ] Confidence calibration validated (ECE < threshold)\n- [ ] Edge cases documented and handled\n- [ ] Performance regression tests passing\n- [ ] Bias audits completed for all segments\n\n### Infrastructure Readiness\n- [ ] Monitoring dashboards operational\n- [ ] Alerting configured and tested\n- [ ] Fallback mechanisms verified\n- [ ] Rollback procedure documented and tested\n- [ ] On-call rotation established\n\n### User Readiness\n- [ ] Documentation and training materials ready\n- [ ] Support team trained on AI feature\n- [ ] FAQ and troubleshooting guides published\n- [ ] Feedback collection mechanism in place\n\n## 3. Phased Rollout Plan\n\n### Phase 0: Shadow Mode (Week 1-2)\n**Goal**: Validate model performance on real production data\n\n- **What**: Model runs on all traffic, outputs logged but not shown\n- **Who**: All users (invisible to them)\n- **Metrics**: Compare AI predictions vs. human decisions\n- **Success criteria**: AI accuracy matches or exceeds baseline\n- **Risk mitigation**: Zero user impact, purely observational\n\n### Phase 1: Internal Dogfood (Week 2-3)\n**Goal**: Gather internal feedback on UX and accuracy\n\n- **What**: Full feature enabled for internal users\n- **Who**: Internal team, beta testers\n- **Metrics**: Usability feedback, edge case discovery\n- **Success criteria**: >80% positive feedback, critical bugs fixed\n- **Risk mitigation**: Known, trusted users, direct feedback channel\n\n### Phase 2: Limited Beta (Week 3-5)\n**Goal**: Validate with real customers in controlled environment\n\n- **What**: Feature available in suggestion mode\n- **Who**: 5-10 hand-selected customers (opted-in)\n- **Metrics**: Accuracy, adoption, support tickets, qualitative feedback\n- **Success criteria**: >90% accuracy, no critical issues\n- **Risk mitigation**: Direct customer relationships, rapid response\n\n### Phase 3: Controlled Rollout (Week 5-8)\n**Goal**: Scale while maintaining quality\n\n- **What**: Gradual percentage rollout with confidence ramp\n- **Who**: 10% → 25% → 50% of eligible users\n- **Metrics**: Full production metrics at scale\n- **Success criteria**: Metrics hold across segments\n- **Risk mitigation**: Automatic rollback on metric degradation\n\n### Phase 4: General Availability (Week 8+)\n**Goal**: Full production deployment\n\n- **What**: Feature available to all users\n- **Who**: 100% of eligible users\n- **Metrics**: Ongoing monitoring\n- **Success criteria**: Sustained metric achievement\n- **Risk mitigation**: Continuous monitoring, rapid response\n\n## 4. Audience Selection Strategy\n\n### Segment Prioritization\nFor each segment, evaluate:\n- **Risk level**: Impact of errors on this segment\n- **Value potential**: Benefit if AI works well\n- **Feedback quality**: Ability to provide useful feedback\n- **Technical sophistication**: Ability to work around issues\n\n### Selection Criteria for Early Access\n- Customers with strong relationship\n- Lower-risk use cases initially\n- Diverse enough to validate across scenarios\n- Engaged enough to provide feedback\n\n### Segment Rollout Order\n1. Internal users (lowest risk, fastest feedback)\n2. SMB segment (lower stakes, faster iteration)\n3. Mid-market (moderate stakes, scaled validation)\n4. Enterprise (highest stakes, proven stability)\n\n## 5. Confidence Thresholds & Routing\n\n### Threshold Strategy\n| Confidence Level | Behavior | User Experience |\n|-----------------|----------|-----------------|\n| >0.95 | Auto-process | No intervention needed |\n| 0.80-0.95 | Suggest & confirm | One-click approval |\n| 0.60-0.80 | Suggest & review | User reviews before confirm |\n| <0.60 | Human required | Route to manual queue |\n\n### Threshold Progression\n- Week 1-2: Conservative (all to human review)\n- Week 3-4: Moderate (high confidence auto-only)\n- Week 5+: Optimized (based on observed performance)\n\n## 6. Feature Flags & Controls\n\n### Rollout Controls\n- **Kill switch**: Instantly disable AI feature globally\n- **Segment toggle**: Enable/disable by customer segment\n- **Percentage rollout**: Control % of traffic using AI\n- **Confidence override**: Adjust thresholds dynamically\n- **Mode selection**: Switch between suggestion/confirmation/auto\n\n### Per-Customer Controls\n- **Opt-out**: Customer can disable AI feature\n- **Threshold adjustment**: Customer can set confidence thresholds\n- **Mode selection**: Customer can choose automation level\n\n## 7. Monitoring & Alerts\n\n### Real-Time Dashboards\n- Model accuracy (rolling 24h)\n- Confidence distribution\n- Processing latency\n- Error rates\n- User adoption/override rates\n\n### Alert Thresholds\n| Metric | Warning | Critical | Action |\n|--------|---------|----------|--------|\n| Accuracy | <93% | <90% | Pause rollout |\n| Latency p95 | >5s | >10s | Scale up |\n| Error rate | >2% | >5% | Rollback |\n| Override rate | >30% | >50% | Investigate |\n\n### Escalation Path\n1. Automated alert → On-call engineer\n2. Investigation (15 min) → ML Lead\n3. Decision point → Product Lead + Engineering Lead\n4. Rollback decision → VP Engineering\n\n## 8. Rollback Procedures\n\n### Automatic Rollback Triggers\n- Accuracy drops below minimum threshold\n- Error rate exceeds critical threshold\n- Service availability below SLA\n- Security incident detected\n\n### Manual Rollback Procedure\n1. Execute kill switch (immediate)\n2. Notify affected users\n3. Route pending work to manual queue\n4. Post-mortem within 24 hours\n\n### Rollback Recovery\n- Root cause analysis\n- Fix validation in shadow mode\n- Gradual re-rollout with closer monitoring\n\n## 9. User Communication\n\n### Pre-Launch\n- Announcement of upcoming feature\n- What to expect, benefits, limitations\n- How to provide feedback\n- Opt-out options\n\n### During Rollout\n- Feature is now available notification\n- Quick start guide\n- Support channel for questions\n\n### Feedback Collection\n- In-product feedback button\n- Regular NPS surveys\n- Support ticket analysis\n- Customer success check-ins\n\n## 10. Success Criteria by Phase\n\nDefine go/no-go criteria for each phase transition.\n",
      "slug": "ai-rollout-strategy",
      "shortDescription": "Plan phased rollout for AI features with shadow mode, confidence ramps, and human-in-the-loop transitions.",
      "surface": "execution",
      "pmStage": "Plans & Playbooks",
      "teamScale": "Enterprise",
      "complexity": "Deep",
      "outputType": "Rollout Plan",
      "jobsToBeDone": [
        "Plan safe AI feature rollout",
        "Design human-to-AI transition",
        "Define rollback criteria and procedures"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Intelligent Document Processing"
        },
        {
          "key": "current_process",
          "label": "Current Process",
          "example": "Manual data entry by AP clerks, 15 min per invoice, 5% error rate"
        },
        {
          "key": "user_segments",
          "label": "User Segments",
          "example": "Enterprise customers (100+ users), Mid-market (10-100 users), SMB (<10 users)"
        },
        {
          "key": "risk_tolerance",
          "label": "Risk Tolerance",
          "example": "Low - financial data, errors cause payment delays and compliance issues"
        },
        {
          "key": "success_metrics",
          "label": "Success Metrics",
          "example": "Accuracy >95%, user adoption >70%, processing time <2 min, user satisfaction >4.0/5"
        },
        {
          "key": "timeline_constraints",
          "label": "Timeline Constraints",
          "example": "Q2 launch target, must be stable before month-end processing peaks"
        }
      ]
    },
    {
      "id": "ai-testing-strategy",
      "title": "AI/ML Testing Strategy",
      "category": "execution",
      "path": "execution/ai-testing-strategy",
      "tags": [
        "ai",
        "ml",
        "testing",
        "qa",
        "validation",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are an ML Quality Assurance specialist designing comprehensive testing strategies for AI/ML features, covering model performance, fairness, robustness, and production validation.\n\n# Context\nWe are developing a testing strategy for **{{ai_feature_name}}**.\nModel type: **{{model_type}}**.\nCritical outcomes: **{{critical_outcomes}}**.\nUser segments: **{{user_segments}}**.\nKnown edge cases: **{{edge_cases}}**.\nRegulatory requirements: **{{regulatory_requirements}}**.\n\n# Task\nCreate a comprehensive ML testing strategy that goes beyond traditional software testing to address the unique challenges of AI systems: probabilistic outputs, bias, data drift, and adversarial robustness.\n\n# Requirements\n\n## 1. Test Data Strategy\n\n### Training/Test Split\n- **Hold-out test set**: Data model has never seen\n- **Temporal split**: Recent data for validation\n- **Stratified sampling**: Representation across segments\n- **Golden test set**: Curated, high-quality examples\n\n### Test Data Requirements\n- **Coverage**: All document types, languages, edge cases\n- **Labeling quality**: Ground truth validation\n- **Size requirements**: Statistically significant samples\n- **Refresh cadence**: How often to update test sets\n\n### Synthetic Data\n- **When to use**: Edge cases, rare scenarios\n- **Generation methods**: Rules-based, generative models\n- **Validation**: Human review of synthetic samples\n\n## 2. Model Accuracy Testing\n\n### Overall Accuracy Metrics\n- **Accuracy**: % correct predictions\n- **Precision/Recall/F1**: Per class and weighted\n- **Mean Average Precision**: For ranking/detection\n- **Confusion matrix analysis**: Error patterns\n\n### Field-Level Testing\nFor extraction tasks:\n- **Per-field accuracy**: Individual field performance\n- **Weighted accuracy**: By field importance\n- **Full-document accuracy**: All fields correct\n\n### Regression Testing\n- **Baseline comparison**: New vs. previous model\n- **Regression threshold**: Acceptable degradation\n- **Sliced analysis**: By segment, by time period\n\n## 3. Confidence Calibration Testing\n\n### Calibration Metrics\n- **Expected Calibration Error (ECE)**: Confidence vs. accuracy gap\n- **Maximum Calibration Error (MCE)**: Worst-case calibration\n- **Reliability diagrams**: Visual calibration analysis\n\n### Threshold Testing\n- **Accuracy at confidence levels**: Is 90% confidence = 90% accuracy?\n- **Coverage vs. accuracy tradeoffs**: Threshold optimization\n- **Uncertainty quantification**: Error bar accuracy\n\n## 4. Bias and Fairness Testing\n\n### Fairness Metrics\n- **Demographic parity**: Equal positive rates across groups\n- **Equalized odds**: Equal TPR/FPR across groups\n- **Predictive parity**: Equal precision across groups\n- **Individual fairness**: Similar inputs → similar outputs\n\n### Segment Analysis\n- **Geographic segments**: Performance by region\n- **User segments**: Performance by customer type\n- **Input segments**: Performance by document type\n- **Language segments**: Performance by language\n\n### Bias Detection\n- **Proxy variable analysis**: Features correlated with protected attributes\n- **Outcome disparity**: Different error rates for different groups\n- **Representation audit**: Training data coverage\n\n## 5. Robustness Testing\n\n### Input Perturbation\n- **Noise injection**: Image quality degradation\n- **Format variations**: Layout changes, fonts\n- **Missing data**: Incomplete inputs\n- **Corrupted data**: Partial file corruption\n\n### Adversarial Testing\n- **Adversarial examples**: Crafted to fool model\n- **Prompt injection**: For LLM components\n- **Evasion attacks**: Designed to cause misclassification\n\n### Boundary Testing\n- **Edge cases**: Minimum/maximum valid inputs\n- **Out-of-distribution**: Inputs unlike training data\n- **Corner cases**: Combinations of edge conditions\n\n## 6. Performance Testing\n\n### Latency Testing\n- **Benchmarking**: p50, p95, p99 latency\n- **Load testing**: Performance under load\n- **Stress testing**: Breaking point identification\n- **Spike testing**: Response to traffic bursts\n\n### Scalability Testing\n- **Horizontal scaling**: Add instances\n- **Vertical scaling**: Increase resources\n- **Cost efficiency**: $/inference at scale\n\n### Resource Testing\n- **Memory usage**: Peak and average\n- **GPU utilization**: Inference efficiency\n- **Batch processing**: Throughput optimization\n\n## 7. Integration Testing\n\n### API Contract Testing\n- **Request validation**: Input schema enforcement\n- **Response validation**: Output schema compliance\n- **Error handling**: Graceful degradation\n- **Timeout behavior**: Fallback mechanisms\n\n### End-to-End Testing\n- **Full workflow**: Upload → inference → display\n- **User journey testing**: Critical paths\n- **Cross-system testing**: Dependencies\n\n### Fallback Testing\n- **Service degradation**: Partial failures\n- **Complete failure**: Model unavailable\n- **Recovery testing**: Restore from failure\n\n## 8. Production Validation\n\n### Shadow Testing\n- **A/B comparison**: Model vs. baseline\n- **Canary deployment**: Small traffic percentage\n- **Blue-green testing**: Parallel environments\n\n### Monitoring Validation\n- **Alert testing**: Verify alerts fire correctly\n- **Dashboard accuracy**: Metrics correctness\n- **Logging completeness**: Required data captured\n\n### Rollback Testing\n- **Rollback procedure**: Verify works correctly\n- **Data consistency**: No corruption on rollback\n- **Recovery time**: Meet RTO requirements\n\n## 9. Compliance Testing\n\n### Regulatory Requirements\n- **Data privacy**: PII handling verification\n- **Audit trails**: Logging completeness\n- **Explainability**: Decision reasoning available\n\n### Documentation Testing\n- **Model cards**: Accuracy and completeness\n- **Bias reports**: Fairness documentation\n- **Limitation disclosure**: User-facing limitations\n\n## 10. Test Automation\n\n### Continuous Testing\n- **CI/CD integration**: Tests in pipeline\n- **Automated regression**: On every model update\n- **Scheduled testing**: Regular validation\n\n### Test Infrastructure\n- **Test data management**: Versioning, access\n- **Evaluation harness**: Standardized testing\n- **Reporting**: Automated metrics tracking\n\n# Output Format\nComprehensive test strategy document with:\n1. Test plan by category\n2. Test case templates\n3. Pass/fail criteria\n4. Automation approach\n5. Reporting requirements\n",
      "slug": "ai-testing-strategy",
      "shortDescription": "Design comprehensive testing strategy for ML models covering accuracy, bias, robustness, and production validation.",
      "surface": "execution",
      "pmStage": "Risk, Quality & Validation",
      "teamScale": "Enterprise",
      "complexity": "Deep",
      "outputType": "Test Strategy",
      "jobsToBeDone": [
        "Design ML model test suites",
        "Plan bias and fairness testing",
        "Define production validation procedures"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Intelligent Document Processing"
        },
        {
          "key": "model_type",
          "label": "Model Type",
          "example": "Multi-modal transformer for document understanding (vision + NLP)"
        },
        {
          "key": "critical_outcomes",
          "label": "Critical Outcomes",
          "example": "Financial field extraction accuracy - errors cause payment issues and compliance risk"
        },
        {
          "key": "user_segments",
          "label": "User Segments to Test",
          "example": "Enterprise (complex docs), Mid-market (standard docs), International (multi-language)"
        },
        {
          "key": "edge_cases",
          "label": "Known Edge Cases",
          "example": "Handwritten notes, poor scan quality, multi-column layouts, non-standard formats"
        },
        {
          "key": "regulatory_requirements",
          "label": "Regulatory Requirements",
          "example": "GDPR (no PII leakage), SOC 2 (audit trails), EU AI Act (bias documentation)"
        }
      ]
    },
    {
      "id": "api-fetch",
      "title": "Live API Data Fetch & Validation",
      "category": "execution",
      "path": "execution/api-fetch",
      "tags": [
        "python",
        "api",
        "requests"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Data Analyst validating third-party API data for a product demo.\n\n# Context\nWe need to confirm the API responds reliably and contains the fields required for a prototype.\n\n# Task\nFetch data from the API and return a concise summary of the first {{record_limit}} records with the requested fields.\n\n# Inputs\n- **API URL**: {{api_url}}\n- **Query params**: {{query_params}}\n- **Fields to extract**: {{fields_to_extract}}\n- **Record limit**: {{record_limit}}\n\n# Requirements\n1. **Request**: Use `requests` with timeout and basic error handling.\n2. **Validation**: Confirm the response is a list of objects and that requested fields exist.\n3. **Output**: Print a tidy table of the extracted fields and a short summary (record count, missing fields).\n4. **PM value**: Note whether the API data is sufficient for the intended demo.\n\n# Output Format\n- Python code\n- Printed table + summary lines\n",
      "slug": "api-fetch",
      "shortDescription": "Pull live API data, validate response shape, and summarize key fields.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Structured",
      "outputType": "Executed Code (Data)",
      "jobsToBeDone": [
        "Verify third-party API reliability",
        "Extract fields needed for a demo or analysis"
      ],
      "inputs": [
        {
          "key": "api_url",
          "label": "API URL",
          "example": "https://jsonplaceholder.typicode.com/todos"
        },
        {
          "key": "query_params",
          "label": "Query Parameters",
          "example": "userId=1&completed=false"
        },
        {
          "key": "fields_to_extract",
          "label": "Fields to Extract",
          "example": "id, title, completed"
        },
        {
          "key": "record_limit",
          "label": "Record Limit",
          "example": "5"
        }
      ]
    },
    {
      "id": "correlation-analysis",
      "title": "Correlation & Relationship Check",
      "category": "execution",
      "path": "execution/correlation-analysis",
      "tags": [
        "python",
        "statistics",
        "correlation"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Analyst validating whether two metrics move together.\n\n# Context\nWe want to know if changes in Metric A explain changes in Metric B.\n\n# Task\nCompute the correlation between the two lists and interpret the strength and direction.\n\n# Inputs\n- **Metric A values**: {{variable_a}}\n- **Metric B values**: {{variable_b}}\n- **Correlation method**: {{method}} (pearson or spearman)\n\n# Requirements\n1. **Validation**: Ensure both lists are numeric and of equal length.\n2. **Calculation**: Compute correlation coefficient and p-value.\n3. **Interpretation**: Classify strength (weak/moderate/strong) and direction (positive/negative).\n4. **PM takeaway**: Note whether the relationship supports a causal hypothesis (with caveats).\n\n# Output Format\n- Python code\n- Printed coefficient, p-value, and interpretation\n",
      "slug": "correlation-analysis",
      "shortDescription": "Quantify the relationship between two product metrics and interpret strength.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "featured": true,
      "complexity": "Structured",
      "outputType": "Executed Code (Statistics)",
      "jobsToBeDone": [
        "Validate metric relationships",
        "Support hypotheses with statistical evidence"
      ],
      "inputs": [
        {
          "key": "variable_a",
          "label": "Metric A Values",
          "example": "[50, 60, 55, 70, 65]"
        },
        {
          "key": "variable_b",
          "label": "Metric B Values",
          "example": "[200, 240, 220, 300, 270]"
        },
        {
          "key": "method",
          "label": "Correlation Method",
          "example": "pearson"
        }
      ]
    },
    {
      "id": "data-cleaning",
      "title": "CSV Cleaning & Quality Report",
      "category": "execution",
      "path": "execution/data-cleaning",
      "tags": [
        "python",
        "pandas",
        "data-cleaning"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Data Analyst preparing raw data for reliable reporting.\n\n# Context\nThe team needs a clean CSV with consistent types and missing values handled before running analysis.\n\n# Task\nLoad the CSV, clean the requested columns, and output a quality report.\n\n# Inputs\n- **CSV path**: {{csv_path}}\n- **Columns to clean**: {{columns_to_fix}}\n- **Imputation strategy**: {{imputation_strategy}}\n\n# Requirements\n1. **Missing values**: Impute numeric columns with median and categorical with mode (unless otherwise specified).\n2. **Type enforcement**: Convert numeric columns to numeric and strip whitespace in categoricals.\n3. **Reporting**: Print before/after missing counts and a preview of cleaned data.\n4. **Output**: Save a cleaned CSV with `_cleaned` suffix.\n\n# Output Format\n- Python code\n- Printed quality report\n",
      "slug": "data-cleaning",
      "shortDescription": "Clean missing values, enforce data types, and report quality changes.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Structured",
      "outputType": "Executed Code (Pandas)",
      "jobsToBeDone": [
        "Prepare data for analysis",
        "Standardize fields for dashboards"
      ],
      "inputs": [
        {
          "key": "csv_path",
          "label": "CSV File Path",
          "example": "user_data.csv"
        },
        {
          "key": "columns_to_fix",
          "label": "Columns to Clean",
          "example": "age, country, plan_tier"
        },
        {
          "key": "imputation_strategy",
          "label": "Imputation Strategy",
          "example": "numeric=median, categorical=mode"
        }
      ]
    },
    {
      "id": "data-simulation",
      "title": "Growth Simulation & Moving Average",
      "category": "execution",
      "path": "execution/data-simulation",
      "tags": [
        "python",
        "simulation",
        "analysis"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Analyst simulating growth scenarios.\n\n# Context\nWe need a quick way to model daily sign-up volatility and communicate trends.\n\n# Task\nSimulate daily sign-ups and compute a moving average trend line.\n\n# Inputs\n- **Days**: {{days}}\n- **Daily mean sign-ups**: {{daily_mean}}\n- **Daily standard deviation**: {{daily_stddev}}\n- **Moving average window**: {{window_size}}\n\n# Requirements\n1. **Simulation**: Generate a daily series using a normal distribution with the provided mean/std dev.\n2. **Trend**: Calculate a moving average with the given window size.\n3. **Summary**: Print mean, min, max, and the latest moving average value.\n4. **PM insight**: Briefly explain what the trend suggests about growth stability.\n\n# Output Format\n- Python code\n- Printed summary lines\n",
      "slug": "data-simulation",
      "shortDescription": "Simulate user growth scenarios and summarize trend smoothing.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "featured": true,
      "complexity": "Structured",
      "outputType": "Executed Code (Simulation)",
      "jobsToBeDone": [
        "Model sign-up volatility",
        "Explain trends with moving averages"
      ],
      "inputs": [
        {
          "key": "days",
          "label": "Number of Days",
          "example": "30"
        },
        {
          "key": "daily_mean",
          "label": "Daily Mean Sign-ups",
          "example": "120"
        },
        {
          "key": "daily_stddev",
          "label": "Daily Std Dev",
          "example": "15"
        },
        {
          "key": "window_size",
          "label": "Moving Average Window",
          "example": "7"
        }
      ]
    },
    {
      "id": "forecasting",
      "title": "User Growth Forecast (with Churn)",
      "category": "execution",
      "path": "execution/forecasting",
      "tags": [
        "python",
        "forecasting",
        "growth"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Strategist forecasting user growth for planning.\n\n# Context\nWe need a clear projection of users over the next planning horizon given growth and churn.\n\n# Task\nForecast the user base over the specified number of months using the provided growth and churn rates.\n\n# Inputs\n- **Starting users**: {{starting_users}}\n- **Monthly growth rate**: {{growth_rate}}\n- **Monthly churn rate**: {{churn_rate}}\n- **Months to forecast**: {{months}}\n\n# Requirements\n1. **Model**: Each month, apply growth then churn to the current base.\n2. **Output**: Print a month-by-month table and final total users.\n3. **Sensitivity**: Highlight the net growth rate (growth - churn).\n4. **PM insight**: Call out whether the trajectory is accelerating or flattening.\n\n# Output Format\n- Python code\n- Printed table and summary\n",
      "slug": "forecasting",
      "shortDescription": "Project user base over time using growth and churn assumptions.",
      "surface": "execution",
      "pmStage": "Plans & Playbooks",
      "complexity": "Deep",
      "outputType": "Executed Code (Projection)",
      "jobsToBeDone": [
        "Estimate future users",
        "Model impact of churn"
      ],
      "inputs": [
        {
          "key": "starting_users",
          "label": "Starting Users",
          "example": "1000"
        },
        {
          "key": "growth_rate",
          "label": "Monthly Growth Rate",
          "example": "8%"
        },
        {
          "key": "churn_rate",
          "label": "Monthly Churn Rate",
          "example": "5%"
        },
        {
          "key": "months",
          "label": "Months to Forecast",
          "example": "12"
        }
      ]
    },
    {
      "id": "metric-calculation",
      "title": "Product Metric Summary",
      "category": "execution",
      "path": "execution/metric-calculation",
      "tags": [
        "python",
        "analysis",
        "statistics"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Analyst preparing a KPI summary for leadership.\n\n# Context\nWe need a quick snapshot of a metric to include in a weekly update.\n\n# Task\nCalculate the requested summary statistics from the metric values.\n\n# Inputs\n- **Metric name**: {{metric_name}}\n- **Metric values**: {{data_list}}\n- **Statistics to compute**: {{metrics_to_calculate}}\n\n# Requirements\n1. **Validation**: Ensure all values are numeric.\n2. **Statistics**: Support mean, median, min, max, and standard deviation.\n3. **Output**: Print a labeled summary block that can be pasted into a report.\n4. **PM insight**: Call out any noticeable volatility if stdev is high.\n\n# Output Format\n- Python code\n- Printed summary block\n",
      "slug": "metric-calculation",
      "shortDescription": "Compute key summary statistics for a product metric series.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Quick",
      "outputType": "Executed Code (Analysis)",
      "jobsToBeDone": [
        "Summarize KPI performance",
        "Share quick metric snapshots"
      ],
      "inputs": [
        {
          "key": "metric_name",
          "label": "Metric Name",
          "example": "daily active users"
        },
        {
          "key": "data_list",
          "label": "Metric Values",
          "example": "[120, 135, 150, 160, 155, 180, 200]"
        },
        {
          "key": "metrics_to_calculate",
          "label": "Statistics",
          "example": "average, median, minimum, maximum"
        }
      ]
    },
    {
      "id": "release-manager",
      "title": "Release Notes & Version Bump",
      "category": "execution",
      "path": "execution/release-manager",
      "tags": [
        "release",
        "git",
        "automation"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Release Manager preparing a shippable release package.\n\n# Context\nWe need concise release notes, a version bump checklist, and a list of risky changes since the last tag.\n\n# Task\nReview git history between {{since_tag}} and HEAD to draft release notes for version {{version}}.\n\n# Inputs\n- **Target version**: {{version}}\n- **Compare against tag**: {{since_tag}}\n- **Release date**: {{release_date}}\n\n# Requirements\n1. **Changelog**: Summarize commits into Features, Fixes, and Chores.\n2. **Risk review**: Call out database, billing, or auth changes explicitly.\n3. **Checklist**: Include a short checklist (tests run, migrations, docs update).\n4. **Version bump**: Identify files that likely need version updates (package.json, changelog, etc.).\n\n# Output Format\n- Release notes markdown\n- Checklist\n",
      "slug": "release-manager",
      "shortDescription": "Generate release notes and checklist from recent git changes.",
      "surface": "execution",
      "pmStage": "Plans & Playbooks",
      "complexity": "Deep",
      "outputType": "Release Package",
      "jobsToBeDone": [
        "Draft release notes fast",
        "Standardize version bumps"
      ],
      "inputs": [
        {
          "key": "version",
          "label": "Target Version",
          "example": "v1.2.0"
        },
        {
          "key": "since_tag",
          "label": "Compare Against Tag",
          "example": "v1.1.0"
        },
        {
          "key": "release_date",
          "label": "Release Date",
          "example": "2026-02-01"
        }
      ]
    },
    {
      "id": "sentiment-analysis",
      "title": "User Feedback Sentiment Triage",
      "category": "execution",
      "path": "execution/sentiment-analysis",
      "tags": [
        "python",
        "nlp",
        "sentiment"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Product Researcher triaging qualitative feedback.\n\n# Context\nWe need a quick sentiment overview to prioritize what to fix or amplify.\n\n# Task\nClassify each feedback entry as positive, negative, or neutral and summarize themes.\n\n# Inputs\n- **Feedback list**: {{feedback_list}}\n- **Positive keywords**: {{positive_keywords}}\n- **Negative keywords**: {{negative_keywords}}\n\n# Requirements\n1. **Classification**: Use keyword matching to label each entry.\n2. **Theme extraction**: List top recurring words in negative feedback.\n3. **Summary**: Provide counts by sentiment and a PM recommendation.\n4. **Output**: Include a small table of feedback with labels.\n\n# Output Format\n- Python code\n- Printed table + summary\n",
      "slug": "sentiment-analysis",
      "shortDescription": "Classify feedback sentiment and surface actionable themes.",
      "surface": "execution",
      "pmStage": "Research & Insight Reports",
      "complexity": "Structured",
      "outputType": "Executed Code (NLP)",
      "jobsToBeDone": [
        "Prioritize feedback",
        "Spot recurring pain points"
      ],
      "inputs": [
        {
          "key": "feedback_list",
          "label": "Feedback List",
          "example": "['Love the new update!', 'The app crashes often.', 'Great customer service.', 'Not intuitive to use.']"
        },
        {
          "key": "positive_keywords",
          "label": "Positive Keywords",
          "example": "love, great, fast, helpful"
        },
        {
          "key": "negative_keywords",
          "label": "Negative Keywords",
          "example": "crash, slow, confusing, not"
        }
      ]
    },
    {
      "id": "task-execution",
      "title": "Task Escalation Automation",
      "category": "execution",
      "path": "execution/task-execution",
      "tags": [
        "python",
        "automation",
        "tasks"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are an Operations Analyst automating task follow-ups.\n\n# Context\nThe PM team needs a quick way to surface overdue items and notify owners.\n\n# Task\nIdentify overdue tasks from the input list and generate reminder messages.\n\n# Inputs\n- **Task data**: {{task_data}}\n- **Today's date**: {{today}}\n\n# Requirements\n1. **Filtering**: Only flag tasks that are open and past due.\n2. **Output**: Print a table of overdue tasks and a reminder message per owner.\n3. **Summary**: Include total tasks, overdue count, and % overdue.\n4. **PM insight**: Recommend whether to escalate based on overdue percentage.\n\n# Output Format\n- Python code\n- Printed table + summary\n",
      "slug": "task-execution",
      "shortDescription": "Flag overdue tasks and generate owner reminders.",
      "surface": "execution",
      "pmStage": "Automation & Scripts",
      "complexity": "Structured",
      "outputType": "Executed Code (Automation)",
      "jobsToBeDone": [
        "Identify overdue work",
        "Nudge owners with reminders"
      ],
      "inputs": [
        {
          "key": "task_data",
          "label": "Task Data",
          "example": "[{\"name\": \"Update pricing\", \"owner\": \"Ava\", \"due_date\": \"2026-01-20\", \"status\": \"open\"}]"
        },
        {
          "key": "today",
          "label": "Today's Date",
          "example": "2026-01-21"
        }
      ]
    },
    {
      "id": "test-plan",
      "title": "Feature Test Plan & Coverage Review",
      "category": "execution",
      "path": "execution/test-plan",
      "tags": [
        "qa",
        "testing",
        "product"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a QA Lead reviewing readiness for a production feature release.\n\n# Context\nThe PM needs a thorough test plan that covers the riskiest areas.\n\n# Task\nCreate a test plan for the feature including test cases and coverage gaps.\n\n# Inputs\n- **Feature description**: {{feature_description}}\n- **Risk areas**: {{risk_areas}}\n- **Dependencies**: {{dependencies}}\n- **Test environment**: {{test_environment}}\n\n# Requirements\n1. **Coverage**: Include unit, integration, and end-to-end tests.\n2. **Edge cases**: Identify at least 5 high-risk edge cases.\n3. **Data setup**: Specify test data and fixture needs.\n4. **Exit criteria**: Define clear go/no-go conditions.\n5. **PM value**: Summarize the remaining risks after testing.\n\n# Output Format\n- Test plan with sections and bullet points\n",
      "slug": "test-plan",
      "shortDescription": "Draft a comprehensive test plan with unit, integration, and E2E coverage.",
      "surface": "execution",
      "pmStage": "Plans & Playbooks",
      "complexity": "Deep",
      "outputType": "Test Plan",
      "jobsToBeDone": [
        "Ensure coverage for a new feature",
        "Reduce regression risk before release"
      ],
      "inputs": [
        {
          "key": "feature_description",
          "label": "Feature Description",
          "example": "Self-serve invoice download for billing admins"
        },
        {
          "key": "risk_areas",
          "label": "Risk Areas",
          "example": "permissions, PDF generation, billing data integrity"
        },
        {
          "key": "dependencies",
          "label": "Dependencies",
          "example": "Stripe API, PDF renderer"
        },
        {
          "key": "test_environment",
          "label": "Test Environment",
          "example": "staging with anonymized billing data"
        }
      ]
    },
    {
      "id": "ux-review",
      "title": "UX Flow Review & Redesign Suggestions",
      "category": "execution",
      "path": "execution/ux-review",
      "tags": [
        "ux",
        "research",
        "product",
        "prototype",
        "poc"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\n\nYou are a Senior Product Designer reviewing a product flow or prototype.\n\n# Context\n\nThe PM team needs a fast, high-quality UX audit to improve the user experience.\n\n**Product/Feature**: {{product_name}}\n**Flow Description**: {{flow_description}}\n**Target Persona**: {{target_persona}}\n**Success Metric**: {{success_metric}}\n**Constraints**: {{constraints}}\n\n{{#if prd_reference}}\n**PRD Reference** (for requirements validation):\n{{prd_reference}}\n{{/if}}\n\n{{#if poc_demo_notes}}\n**POC/Prototype Notes** (what was built):\n{{poc_demo_notes}}\n{{/if}}\n\n# Task\n\nReview the described flow or prototype, identify friction points, and propose improvements.\n\n# Requirements\n\n## 1. Friction Audit\nList top 5 friction points tied to the persona:\n- What's confusing or unclear?\n- Where do users hesitate or drop off?\n- What's missing that users expect?\n- What's unnecessary or overwhelming?\n\n## 2. Requirements Validation (if PRD provided)\nCheck the UX against stated requirements:\n- Which user stories are well-supported?\n- Which user stories have UX gaps?\n- Any requirements that conflict with good UX?\n\n## 3. Recommendations\nProvide 3–5 UX changes with:\n- Clear description of the change\n- Rationale tied to user needs\n- Impact/Effort rating\n\n## 4. Prioritized Action Table\n\n| Change | Impact | Effort | Priority | Notes |\n|--------|--------|--------|----------|-------|\n| ... | High/Med/Low | High/Med/Low | P0/P1/P2 | ... |\n\n## 5. Acceptance Criteria\nFor top 2 changes, provide testable criteria:\n- What does \"done\" look like?\n- How would you verify the improvement?\n\n## 6. Quick Wins vs. Post-POC\nSeparate recommendations into:\n- **Quick Wins**: Can fix in hours, high impact\n- **Pre-Launch**: Should fix before GA\n- **Post-Launch**: Nice to have, can iterate\n\n# Output Format\n\nReturn the entire UX review as a **single markdown code block** so it can be easily copied and pasted. Use headings and tables for clarity.\n",
      "slug": "ux-review",
      "shortDescription": "Audit a user flow or prototype for friction points and propose improvements.",
      "surface": "execution",
      "pmStage": "Research & Insight Reports",
      "complexity": "Structured",
      "outputType": "UX Review Report",
      "jobsToBeDone": [
        "Identify friction in key flows",
        "Validate POC/prototype against requirements",
        "Prioritize UX fixes with clear acceptance criteria"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product/Feature Name",
          "example": "AI Support Chatbot"
        },
        {
          "key": "flow_description",
          "label": "Flow Description",
          "example": "User opens chat widget, types question, receives AI response, can escalate to human"
        },
        {
          "key": "target_persona",
          "label": "Target Persona",
          "example": "Customer seeking quick answers to billing/shipping questions"
        },
        {
          "key": "success_metric",
          "label": "Success Metric",
          "example": "80% of queries resolved without human escalation"
        },
        {
          "key": "constraints",
          "label": "Constraints",
          "example": "Must work on mobile; POC timeline is 3 days"
        },
        {
          "key": "prd_reference",
          "label": "PRD Reference (optional)",
          "example": "Paste key user stories or requirements from your PRD to validate against",
          "optional": true
        },
        {
          "key": "poc_demo_notes",
          "label": "POC/Prototype Notes (optional)",
          "example": "Describe what was built: 'Chat widget with streaming responses, FAQ search, escalate button'",
          "optional": true
        }
      ]
    },
    {
      "id": "visualization",
      "title": "Metric Trend Visualization",
      "category": "execution",
      "path": "execution/visualization",
      "tags": [
        "python",
        "matplotlib",
        "visualization"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Data Storyteller creating KPI charts for stakeholder updates.\n\n# Context\nWe need a quick visual to explain a trend and highlight a key moment.\n\n# Task\nParse the provided data points and generate the requested chart type.\n\n# Inputs\n- **Chart type**: {{chart_type}}\n- **Data points**: {{data_points}}\n- **Highlight point**: {{highlight_point}}\n\n# Requirements\n1. **Parsing**: Convert the label:value pairs into ordered data.\n2. **Visualization**: Render the chart with labels and a highlighted annotation for the chosen point.\n3. **Output**: Save the chart as `kpi_chart.png`.\n4. **PM insight**: Print a one-line interpretation of the highlighted change.\n\n# Output Format\n- Python code\n- Chart image saved + printed insight line\n",
      "slug": "visualization",
      "shortDescription": "Create a chart for a KPI trend and annotate key changes.",
      "surface": "execution",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Structured",
      "outputType": "Executed Code (Chart)",
      "jobsToBeDone": [
        "Visualize KPI performance",
        "Create shareable charts"
      ],
      "inputs": [
        {
          "key": "chart_type",
          "label": "Chart Type",
          "example": "line"
        },
        {
          "key": "data_points",
          "label": "Data Points",
          "example": "Jan: 100, Feb: 150, Mar: 130"
        },
        {
          "key": "highlight_point",
          "label": "Highlight Point",
          "example": "Feb"
        }
      ]
    },
    {
      "id": "ai-model-requirements",
      "title": "AI Model Performance Requirements & SLAs",
      "category": "prototyping",
      "path": "prototyping/ai-model-requirements",
      "tags": [
        "ai",
        "ml",
        "metrics",
        "sla",
        "performance",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are an ML Product Manager defining production requirements for an AI feature, bridging business needs with technical model capabilities.\n\n# Context\nWe are defining requirements for **{{ai_feature_name}}**.\nML task: **{{task_type}}**.\nBusiness context: **{{business_context}}**.\nVolume: **{{volume_expectations}}**.\nError tolerance: **{{error_tolerance}}**.\nUser workflow: **{{user_workflow}}**.\n\n# Task\nCreate a comprehensive model requirements specification that defines success criteria, performance thresholds, and SLAs that engineering can build against and business can measure.\n\n# Requirements\n\n## 1. Business Success Metrics\n\nDefine metrics that matter to business stakeholders:\n- **Primary outcome metric**: The business KPI this AI feature improves\n- **Efficiency gains**: Time/cost savings quantified\n- **Quality improvements**: Error reduction, consistency gains\n- **User adoption targets**: Usage and satisfaction goals\n\n## 2. Model Performance Metrics\n\n### Classification/Detection Tasks\n- **Accuracy**: Overall correctness rate\n- **Precision**: Of predicted positives, how many are correct?\n- **Recall**: Of actual positives, how many did we find?\n- **F1 Score**: Harmonic mean of precision and recall\n- **Specificity**: True negative rate\n\n### Extraction/Generation Tasks\n- **Exact match rate**: Perfectly correct extractions\n- **Character error rate**: For OCR/transcription\n- **BLEU/ROUGE scores**: For text generation\n- **Semantic accuracy**: Meaning preserved even if words differ\n\n### Confidence & Calibration\n- **Confidence calibration**: Is 90% confidence actually 90% accurate?\n- **Expected calibration error (ECE)**: Calibration gap measurement\n- **Confidence distribution**: Spread of confidence scores\n\n## 3. Performance Thresholds\n\nFor each metric, define:\n- **Minimum viable**: Below this, don't ship\n- **Launch target**: Acceptable for initial launch\n- **Excellence target**: Goal for mature product\n- **Regression threshold**: Drop triggers investigation\n\nConsider thresholds for:\n- Overall performance\n- Performance by segment (user type, input type, etc.)\n- Edge cases and failure modes\n\n## 4. Latency Requirements\n\n- **p50 latency**: Median response time\n- **p95 latency**: 95th percentile (most users)\n- **p99 latency**: 99th percentile (worst case)\n- **Timeout threshold**: When to abort and fallback\n- **User-perceived latency**: Including UI, not just model\n\n## 5. Throughput & Scalability\n\n- **Sustained throughput**: Requests per second, normal load\n- **Peak throughput**: Maximum capacity needed\n- **Burst handling**: How to handle traffic spikes\n- **Scaling characteristics**: Linear, step-function, etc.\n\n## 6. Availability & Reliability\n\n- **Uptime SLA**: 99.9%? 99.95%?\n- **Planned maintenance windows**: When and how long\n- **Degradation modes**: What happens at partial capacity\n- **Fallback behavior**: What if AI is unavailable\n\n## 7. Cost Constraints\n\n- **Cost per inference**: Target unit economics\n- **Monthly budget ceiling**: Maximum spend\n- **Cost vs. accuracy tradeoffs**: Where to optimize\n- **ROI requirements**: Payback period expectations\n\n## 8. Segmented Requirements\n\nDefine different thresholds for:\n- **High-value vs. routine**: Critical decisions vs. bulk processing\n- **Enterprise tiers**: SLA differences by customer segment\n- **Regions/languages**: Performance variation acceptable?\n- **Input complexity**: Simple vs. complex inputs\n\n## 9. Monitoring & Alerting Requirements\n\n- **Real-time dashboards**: What metrics to surface\n- **Alerting thresholds**: When to page on-call\n- **Reporting cadence**: Daily/weekly/monthly reviews\n- **Drift detection**: How to spot degradation\n\n## 10. Acceptance Criteria\n\nDefine clear criteria for:\n- **Model readiness**: When is model ready for testing?\n- **Feature readiness**: When is feature ready for beta?\n- **Launch readiness**: When can we GA?\n- **Ongoing compliance**: How do we stay in compliance?\n\n# Output Format\nStructured requirements document with tables for metrics and thresholds, clear definitions, and rationale for each requirement.\n",
      "slug": "ai-model-requirements",
      "shortDescription": "Define ML model metrics, thresholds, and service level agreements for production AI.",
      "surface": "prototyping",
      "pmStage": "Requirements & Specifications",
      "teamScale": "Enterprise",
      "complexity": "Deep",
      "outputType": "Requirements Specification",
      "jobsToBeDone": [
        "Define success criteria for AI features",
        "Set model performance thresholds",
        "Establish SLAs with stakeholders"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Invoice Data Extraction"
        },
        {
          "key": "task_type",
          "label": "ML Task Type",
          "example": "Document extraction with NER and classification"
        },
        {
          "key": "business_context",
          "label": "Business Context",
          "example": "Automate accounts payable processing for enterprise finance teams"
        },
        {
          "key": "volume_expectations",
          "label": "Volume Expectations",
          "example": "10,000 invoices/day, peak 500/hour, avg document 3 pages"
        },
        {
          "key": "error_tolerance",
          "label": "Error Tolerance",
          "example": "Errors cause payment delays and vendor disputes; high accuracy required"
        },
        {
          "key": "user_workflow",
          "label": "User Workflow",
          "example": "AI extracts, human reviews exceptions, auto-routes high-confidence results"
        }
      ]
    },
    {
      "id": "ai-service-api",
      "title": "AI Service API Design",
      "category": "prototyping",
      "path": "prototyping/ai-service-api",
      "tags": [
        "ai",
        "ml",
        "api",
        "inference",
        "async",
        "enterprise"
      ],
      "updatedAt": "2026-01-27T22:49:49+02:00",
      "prompt": "# Role\nYou are an API architect specializing in AI/ML inference services, designing production-ready API contracts that handle the unique challenges of ML systems.\n\n# Context\nWe are designing the API for **{{ai_feature_name}}**.\nInference type: **{{inference_type}}**.\nInput types: **{{input_types}}**.\nOutput format: **{{output_format}}**.\nLatency requirements: **{{latency_requirements}}**.\nAPI consumers: **{{consumers}}**.\n\n# Task\nDesign a comprehensive API specification for an AI inference service that handles ML-specific concerns like confidence scores, model versioning, async processing, and graceful degradation.\n\n# Requirements\n\n## 1. Core Inference Endpoint Design\n\n### Request Structure\n- **Input payload**: How to submit data for inference\n- **Preprocessing options**: Client-controlled preprocessing parameters\n- **Inference parameters**: Confidence thresholds, model selection, output options\n- **Idempotency**: Request deduplication for retries\n\n### Response Structure\n- **Predictions**: Primary model output\n- **Confidence scores**: Per-prediction and aggregate confidence\n- **Metadata**: Processing time, model version, request ID\n- **Warnings**: Non-fatal issues (low confidence, partial results)\n\n## 2. Async Processing Patterns\n\n### For Long-Running Inference\n- **Job submission**: POST to create async job\n- **Status polling**: GET job status endpoint\n- **Webhook callbacks**: Push notification on completion\n- **Result retrieval**: GET completed results\n- **Job cancellation**: DELETE to cancel pending jobs\n\n### Streaming Responses\n- **Server-Sent Events (SSE)**: For incremental results\n- **WebSocket**: For bidirectional streaming\n- **Chunked responses**: For large result sets\n\n## 3. Model Versioning Strategy\n\n### Version Management\n- **Model version header**: X-Model-Version request/response\n- **Version pinning**: Allow clients to pin specific versions\n- **Deprecation policy**: Sunset headers and migration timeline\n- **A/B routing**: Traffic splitting between model versions\n\n### Backward Compatibility\n- **Response schema evolution**: Additive changes only\n- **Feature flags**: New capabilities behind flags\n- **Migration guides**: Version upgrade documentation\n\n## 4. Error Handling for ML Systems\n\n### ML-Specific Error Types\n- **Low confidence**: Model uncertain, recommend human review\n- **Out of distribution**: Input unlike training data\n- **Model unavailable**: Inference service down\n- **Timeout**: Processing exceeded time limit\n- **Quota exceeded**: Rate or usage limits hit\n\n### Error Response Format\n```json\n{\n  \"error\": {\n    \"code\": \"LOW_CONFIDENCE\",\n    \"message\": \"Model confidence below threshold\",\n    \"details\": {\n      \"confidence\": 0.45,\n      \"threshold\": 0.70,\n      \"recommendation\": \"human_review\"\n    },\n    \"request_id\": \"req_123\",\n    \"model_version\": \"v2.1.0\"\n  }\n}\n```\n\n## 5. Confidence & Uncertainty\n\n### Confidence Reporting\n- **Per-field confidence**: Individual prediction confidence\n- **Aggregate confidence**: Overall result reliability\n- **Calibration info**: Is 80% confidence actually 80% accurate?\n\n### Uncertainty Handling\n- **Threshold parameters**: Client-configurable confidence thresholds\n- **Fallback behavior**: What to return when uncertain\n- **Human-in-the-loop routing**: Flag for manual review\n\n## 6. Rate Limiting & Quotas\n\n### Limit Types\n- **Requests per second**: Burst protection\n- **Requests per day**: Usage quotas\n- **Concurrent requests**: Parallel processing limits\n- **Payload size**: Input size restrictions\n\n### Limit Communication\n- **Rate limit headers**: X-RateLimit-Limit, X-RateLimit-Remaining\n- **Quota headers**: X-Quota-Limit, X-Quota-Used\n- **Retry-After**: When to retry after limit hit\n\n## 7. Observability & Debugging\n\n### Request Tracing\n- **Request ID**: Unique identifier for tracing\n- **Correlation ID**: Link related requests\n- **Timing breakdown**: Preprocessing, inference, postprocessing\n\n### Debug Mode\n- **Verbose responses**: Additional debugging info\n- **Feature attribution**: Why did the model decide this?\n- **Intermediate results**: Step-by-step processing\n\n## 8. Security Considerations\n\n### Authentication & Authorization\n- **API key management**: Key rotation, scoping\n- **OAuth 2.0**: For user-context requests\n- **Request signing**: For sensitive operations\n\n### Data Protection\n- **PII handling**: Redaction options\n- **Data retention**: How long inputs/outputs stored\n- **Encryption**: In-transit and at-rest requirements\n\n## 9. SDK & Client Considerations\n\n### Client Libraries\n- **Retry logic**: Exponential backoff with jitter\n- **Timeout handling**: Configurable timeouts\n- **Connection pooling**: Efficient resource usage\n- **Async support**: Native async/await patterns\n\n### Developer Experience\n- **OpenAPI/Swagger spec**: Machine-readable API definition\n- **Code samples**: Examples in major languages\n- **Sandbox environment**: Testing without production impact\n\n# Output Format\nComplete API specification with:\n1. OpenAPI 3.0 schema for all endpoints\n2. Request/response examples\n3. Error code reference\n4. Rate limiting documentation\n5. SDK integration guide\n",
      "slug": "ai-service-api",
      "shortDescription": "Design production-ready API contracts for AI/ML inference services with proper error handling, versioning, and async patterns.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "teamScale": "Enterprise",
      "complexity": "Deep",
      "outputType": "API Specification",
      "jobsToBeDone": [
        "Design AI inference API contracts",
        "Handle async and streaming responses",
        "Plan for model versioning and fallbacks"
      ],
      "inputs": [
        {
          "key": "ai_feature_name",
          "label": "AI Feature Name",
          "example": "Document Intelligence API"
        },
        {
          "key": "inference_type",
          "label": "Inference Type",
          "example": "Synchronous with async fallback for large documents"
        },
        {
          "key": "input_types",
          "label": "Input Types",
          "example": "PDF documents (up to 100 pages), images (PNG, JPG), scanned documents"
        },
        {
          "key": "output_format",
          "label": "Output Format",
          "example": "Structured JSON with extracted fields, confidence scores, and bounding boxes"
        },
        {
          "key": "latency_requirements",
          "label": "Latency Requirements",
          "example": "p95 < 5s for single page, async for multi-page with webhook callback"
        },
        {
          "key": "consumers",
          "label": "API Consumers",
          "example": "Internal web app, mobile SDK, enterprise customers via REST API"
        }
      ]
    },
    {
      "id": "api-endpoint",
      "title": "REST API List Endpoint (Node.js)",
      "category": "prototyping",
      "path": "prototyping/api-endpoint",
      "tags": [
        "node",
        "express",
        "api"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Backend Developer specializing in Node.js and Express.\n\n# Context\nWe need a realistic GET endpoint that supports basic filtering and pagination for a product demo.\n\n# Task\nCreate a REST endpoint `GET /{{resource_name}}` that returns a list of {{resource_name}}.\n\n# Inputs\n- **Resource Name**: {{resource_name}}\n- **Fields**: {{fields}}\n- **Query Parameters**: {{query_params}}\n\n# Requirements\n1. **Framework**: Use Express.js with a single in-memory array as mock data.\n2. **Filtering**: Support filtering using the provided query parameters (ignore `limit`/`offset` for filtering).\n3. **Pagination**: Support `limit` and `offset` query params with safe defaults.\n4. **Response Shape**: Return `{ success, count, data, pagination }`.\n5. **Errors**: Validate `limit` and `offset` to be non-negative integers and return a 400 with a helpful message if invalid.\n\n# Output Format\nProvide a Node.js code snippet (server + route handler).\n",
      "slug": "api-endpoint",
      "shortDescription": "Design a production-ready GET endpoint with filtering and pagination.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Structured",
      "outputType": "Code (Node.js)",
      "jobsToBeDone": [
        "Mock backend logic",
        "Create sample API",
        "Prototype filtering and pagination"
      ],
      "inputs": [
        {
          "key": "resource_name",
          "label": "Resource Name",
          "example": "tasks"
        },
        {
          "key": "fields",
          "label": "Fields",
          "example": "id, title, status, assignee"
        },
        {
          "key": "query_params",
          "label": "Query Parameters",
          "example": "status, assignee, limit, offset"
        }
      ]
    },
    {
      "id": "automation-script",
      "title": "Task Reminder Automation (Python)",
      "category": "prototyping",
      "path": "prototyping/automation-script",
      "tags": [
        "python",
        "automation",
        "productivity"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Python Developer focused on automation.\n\n# Context\nWe want a lightweight reminder script that can be run daily (cron/GitHub Actions) to alert owners about tasks due soon.\n\n# Task\nCreate a Python script that scans {{task_source}} and prints reminders for tasks due within the next {{lead_time_hours}} hours in the {{timezone}} timezone.\n\n# Requirements\n1. **Input**: Accept a list of dictionaries and include a small sample dataset in the script.\n2. **Time Handling**: Parse ISO dates, localize to the provided timezone, and compare against \"now\".\n3. **Output**: Print a concise reminder line per task (include task name, owner, and due date).\n4. **Extensibility**: Isolate a `send_reminder()` function as a placeholder for email/Slack integration.\n5. **Summary**: Print a total count of reminders sent.\n\n# Output Format\nPython script.\n",
      "slug": "automation-script",
      "shortDescription": "Automate due-soon reminders with time-zone aware scheduling.",
      "surface": "prototyping",
      "pmStage": "Automation & Scripts",
      "complexity": "Structured",
      "outputType": "Code (Python)",
      "jobsToBeDone": [
        "Automate repetitive tasks",
        "Send reminders",
        "Generate daily summaries"
      ],
      "inputs": [
        {
          "key": "task_source",
          "label": "Task Source",
          "example": "list of dicts with task_name, due_date, and owner"
        },
        {
          "key": "timezone",
          "label": "Timezone",
          "example": "America/Los_Angeles"
        },
        {
          "key": "lead_time_hours",
          "label": "Lead Time (Hours)",
          "example": "24"
        }
      ]
    },
    {
      "id": "browser-extension",
      "title": "Browser Extension Starter (Manifest V3)",
      "category": "prototyping",
      "path": "prototyping/browser-extension",
      "tags": [
        "extension",
        "chrome",
        "firefox"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Browser Extension Developer.\n\n# Context\nWe need a quick POC extension for {{use_case}} that works across modern browsers.\n\n# Task\nCreate a Manifest V3 extension called \"{{extension_name}}\" targeting {{target_sites}}.\n\n# Requirements\n1. **Manifest**: Include permissions ({{permissions}}), host permissions, icons, and action popup.\n2. **Popup UI**: Provide a clean popup UI that toggles the extension on/off.\n3. **Content Script**: Inject a script to modify the page (e.g., highlight elements) on {{target_sites}}.\n4. **Storage**: Persist user preferences with `chrome.storage.sync` (or `browser.storage.sync`).\n5. **Background**: Include a service worker for handling events and messaging.\n6. **Compatibility Notes**: Briefly note how to adapt for Firefox (manifest version and APIs).\n\n# Output Format\nProvide a file tree and the core files (`manifest.json`, `popup.html`, `popup.js`, `content.js`, `background.js`, `styles.css`).\n",
      "slug": "browser-extension",
      "shortDescription": "Create a POC browser extension with popup, content script, and storage.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Deep",
      "outputType": "Code (Browser Extension)",
      "jobsToBeDone": [
        "Prototype extension workflows",
        "Validate content script UX",
        "Test browser APIs"
      ],
      "inputs": [
        {
          "key": "extension_name",
          "label": "Extension Name",
          "example": "TabTamer"
        },
        {
          "key": "use_case",
          "label": "Use Case",
          "example": "highlight overdue tasks in project tools"
        },
        {
          "key": "target_sites",
          "label": "Target Sites",
          "example": "https://app.asana.com/*"
        },
        {
          "key": "permissions",
          "label": "Permissions",
          "example": "storage, tabs, scripting"
        }
      ]
    },
    {
      "id": "data-script",
      "title": "CSV Insight Generator (Python)",
      "category": "prototyping",
      "path": "prototyping/data-script",
      "tags": [
        "python",
        "csv",
        "data-processing"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Python Developer or Data Engineer.\n\n# Context\nWe have a CSV export and need a quick analysis script that cleans data and produces a summary table.\n\n# Task\nWrite a Python script that reads a CSV with columns: {{csv_columns}} and summarizes {{metric_column}} by {{group_by_column}}.\n\n# Requirements\n1. **Input**: Read from a file path variable (e.g., `INPUT_CSV`).\n2. **Cleaning**: Drop rows with missing {{group_by_column}} or {{metric_column}}; coerce {{metric_column}} to float.\n3. **Aggregation**: Compute count, average, and 90th percentile for {{metric_column}} grouped by {{group_by_column}}.\n4. **Output**: Print the top 5 groups by count and write a `summary.csv` with all groups.\n5. **Errors**: Handle file-not-found and bad columns gracefully.\n\n# Output Format\nPython script.\n",
      "slug": "data-script",
      "shortDescription": "Clean a CSV and summarize key metrics by group.",
      "surface": "prototyping",
      "pmStage": "Data Analysis & Visualizations",
      "complexity": "Structured",
      "outputType": "Code (Python)",
      "jobsToBeDone": [
        "Analyze CSV files",
        "Extract insights from data",
        "Create clean output files"
      ],
      "inputs": [
        {
          "key": "csv_columns",
          "label": "CSV Columns",
          "example": "ticket_id, issue_type, created_at, resolution_hours"
        },
        {
          "key": "group_by_column",
          "label": "Group By Column",
          "example": "issue_type"
        },
        {
          "key": "metric_column",
          "label": "Metric Column",
          "example": "resolution_hours"
        }
      ]
    },
    {
      "id": "db-schema",
      "title": "Relational Schema Blueprint (SQL)",
      "category": "prototyping",
      "path": "prototyping/db-schema",
      "tags": [
        "sql",
        "database",
        "schema"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Database Architect.\n\n# Context\nWe need a clean, production-ready relational schema to guide initial development.\n\n# Task\nDraft a SQL schema for a {{app_type}} with entities: {{core_entities}}.\n\n# Requirements\n1. **Relationships**: Model the relationships described: {{relationships}}.\n2. **Normalization**: Use separate tables and foreign keys instead of repeated fields.\n3. **Constraints**: Include NOT NULL, UNIQUE where appropriate, and CHECK constraints for enums.\n4. **Timestamps**: Add `created_at` and `updated_at` to core tables.\n5. **Indexes**: Add indexes for foreign keys and high-traffic query fields.\n\n# Output Format\nSQL `CREATE TABLE` statements (PostgreSQL dialect) with indexes.\n",
      "slug": "db-schema",
      "shortDescription": "Draft a normalized schema with constraints and indexes.",
      "surface": "prototyping",
      "pmStage": "Code Templates & Schemas",
      "complexity": "Deep",
      "outputType": "Code (SQL)",
      "jobsToBeDone": [
        "Design data models",
        "Create database tables",
        "Capture relationships clearly"
      ],
      "inputs": [
        {
          "key": "app_type",
          "label": "Application Type",
          "example": "bug-tracking system"
        },
        {
          "key": "core_entities",
          "label": "Core Entities",
          "example": "Users, Bugs, Comments, Projects"
        },
        {
          "key": "relationships",
          "label": "Relationships",
          "example": "Users report Bugs; Bugs belong to Projects; Comments belong to Bugs"
        }
      ]
    },
    {
      "id": "electron-app",
      "title": "Electron App Starter (Cross-Platform)",
      "category": "prototyping",
      "path": "prototyping/electron-app",
      "tags": [
        "electron",
        "desktop",
        "cross-platform"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are an Electron Developer shipping cross-platform desktop apps.\n\n# Context\nWe want a rich client POC that runs on macOS, Windows, and Linux for stakeholder demos.\n\n# Task\nCreate a minimal but complete Electron app scaffold for \"{{app_name}}\" that demonstrates {{core_features}}.\n\n# Inputs\n- **Core Features**: {{core_features}}\n- **User Workflows**: {{user_workflows}}\n- **Data Storage**: {{data_storage}}\n\n# Requirements\n1. **Architecture**: Include `main` process, `preload` bridge, and a `renderer` UI.\n2. **Functionality**: Describe how each workflow should behave end-to-end (UI -> data -> UI).\n3. **IPC**: Demonstrate one IPC call (renderer -> main) for reading/writing {{data_storage}}.\n4. **State**: Store data locally and show how it loads on app start.\n5. **UX**: Provide a polished UI layout (header, primary action, list view, empty state).\n6. **Security**: Use context isolation and disable remote module usage.\n7. **Packaging**: Include multi-target build notes and config placeholders for macOS, Windows, and Linux.\n\n# Output Format\nProvide a file tree and the key files (`main.js`, `preload.js`, `renderer.js`, `index.html`, `styles.css`, `package.json`).\n",
      "slug": "electron-app",
      "shortDescription": "Scaffold a cross-platform Electron app with IPC, local storage, and packaging.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Deep",
      "outputType": "Code (Electron)",
      "jobsToBeDone": [
        "Prototype desktop apps",
        "Validate POC workflows",
        "Package cross-platform builds"
      ],
      "inputs": [
        {
          "key": "app_name",
          "label": "App Name",
          "example": "FocusFlow"
        },
        {
          "key": "core_features",
          "label": "Core Features",
          "example": "task list, focus timer, daily summary"
        },
        {
          "key": "user_workflows",
          "label": "User Workflows",
          "example": "create a task, start a focus session, review daily summary"
        },
        {
          "key": "data_storage",
          "label": "Data Storage",
          "example": "local JSON file with autosave"
        }
      ]
    },
    {
      "id": "mobile-model",
      "title": "Codable Mobile Model (Swift)",
      "category": "prototyping",
      "path": "prototyping/mobile-model",
      "tags": [
        "swift",
        "ios",
        "mobile"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are an iOS Developer.\n\n# Context\nWe need a Swift model that can decode API responses and work well with SwiftUI.\n\n# Task\nDefine a Swift struct named `{{struct_name}}` with properties: {{properties}}.\n\n# Requirements\n1. **Protocols**: Conform to `Codable`, `Identifiable`, and `Equatable`.\n2. **JSON Mapping**: Include `CodingKeys` to map snake_case keys from this JSON example: {{json_example}}.\n3. **Date Parsing**: Include a `static` JSON decoder configured for ISO-8601 dates.\n4. **Usage**: Provide a short example of decoding the JSON into the model.\n\n# Output Format\nSwift code snippet.\n",
      "slug": "mobile-model",
      "shortDescription": "Define a Swift model with Codable support and JSON mapping.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Quick",
      "outputType": "Code (Swift)",
      "jobsToBeDone": [
        "Model mobile data",
        "Create iOS structs",
        "Parse API responses"
      ],
      "inputs": [
        {
          "key": "struct_name",
          "label": "Struct Name",
          "example": "UserProfile"
        },
        {
          "key": "properties",
          "label": "Properties",
          "example": "id: String, name: String, email: String, signupDate: Date, avatarUrl: URL?"
        },
        {
          "key": "json_example",
          "label": "JSON Example",
          "example": "{ \"id\": \"u_123\", \"name\": \"Alex\", \"email\": \"alex@example.com\", \"signup_date\": \"2024-01-15T10:00:00Z\", \"avatar_url\": null }"
        }
      ]
    },
    {
      "id": "poc-implementation-guide",
      "title": "POC Implementation Guide",
      "category": "prototyping",
      "path": "prototyping/poc-implementation-guide",
      "tags": [
        "poc",
        "prototype",
        "implementation",
        "planning",
        "rapid"
      ],
      "updatedAt": "2026-01-28T13:56:16.062Z",
      "prompt": "# Role\n\nYou are a Senior Technical PM and rapid prototyping expert who helps teams build POCs quickly and effectively.\n\n# Context\n\nWe are building a POC for **{{product_name}}**.\n\n**PRD Summary**:\n{{prd_summary}}\n\n**Tech Design Summary**:\n{{tech_design_summary}}\n\n**POC Goal**: {{poc_goal}}\n**Timeline**: {{timeline}}\n**Available Resources**: {{team_resources}}\n{{#if existing_assets}}\n**Existing Assets**: {{existing_assets}}\n{{/if}}\n\n# Task\n\nCreate a detailed implementation guide that helps the team build this POC quickly while validating the core hypothesis.\n\n# Requirements\n\n## 1. POC Scope Definition\n\nClearly define:\n- **In Scope**: What MUST be built to validate the POC goal\n- **Out of Scope**: What to explicitly skip (with rationale)\n- **Shortcuts Allowed**: Where to cut corners (mocks, hardcoded data, manual steps)\n\n## 2. Technical Decisions\n\nFor each major component, recommend:\n- **Build vs. Buy vs. Mock**: What to implement, what to use off-the-shelf, what to fake\n- **Tech Stack Choices**: Fastest path to working prototype (prioritize speed over perfection)\n- **Data Strategy**: Real data, synthetic data, or hardcoded fixtures\n\n## 3. Implementation Phases\n\nBreak down into phases with clear milestones:\n\n### Phase 1: Foundation (Day 1)\n- Environment setup\n- Core integrations\n- Basic data flow\n\n### Phase 2: Core Functionality (Day 2-3)\n- Main feature implementation\n- Happy path working\n\n### Phase 3: Polish & Demo-Ready (Day 4-5)\n- Error handling for demo scenarios\n- Basic UI/UX polish\n- Demo script preparation\n\n## 4. Task Breakdown\n\nProvide a detailed task list:\n- Task name and description\n- Estimated effort (hours)\n- Owner suggestion (role)\n- Dependencies\n- Acceptance criteria (minimal)\n\n## 5. Risk Mitigation\n\nIdentify:\n- **Blockers**: What could stop progress entirely\n- **Fallback Plans**: Plan B for each blocker\n- **Daily Checkpoints**: What should be working by end of each day\n\n## 6. Demo Preparation\n\n- Key scenarios to demonstrate\n- Data/setup needed for demo\n- Known limitations to disclose\n- Talking points for stakeholders\n\n# Output Format\n\nReturn the entire implementation guide as a **single markdown code block** so it can be easily copied and pasted. Use clear headings, tables for task breakdown, and checklists where appropriate.\n",
      "slug": "poc-implementation-guide",
      "shortDescription": "Generate a step-by-step implementation plan for building a POC quickly.",
      "surface": "prototyping",
      "pmStage": "Code Templates & Schemas",
      "complexity": "Structured",
      "outputType": "Implementation Plan",
      "jobsToBeDone": [
        "Plan POC implementation steps",
        "Identify what to build vs. skip",
        "Define milestones and checkpoints",
        "Align team on build approach"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product/Feature Name",
          "example": "AI Support Chatbot"
        },
        {
          "key": "prd_summary",
          "label": "PRD Summary",
          "example": "Paste your PRD or key sections: problem, goals, core features, constraints"
        },
        {
          "key": "tech_design_summary",
          "label": "Tech Design Summary",
          "example": "Paste your tech design or key sections: stack, architecture, data model"
        },
        {
          "key": "poc_goal",
          "label": "POC Goal",
          "example": "Validate that LLM can accurately answer FAQ questions with <3s latency"
        },
        {
          "key": "timeline",
          "label": "Timeline",
          "example": "3-5 days"
        },
        {
          "key": "team_resources",
          "label": "Available Resources",
          "example": "1 backend dev, 1 frontend dev, designer for 4 hours"
        },
        {
          "key": "existing_assets",
          "label": "Existing Assets to Leverage",
          "example": "Existing React component library, FAQ database, Slack workspace for testing",
          "optional": true
        }
      ]
    },
    {
      "id": "prd-writer",
      "title": "PRD Generator (MVP Scope)",
      "category": "prototyping",
      "path": "prototyping/prd-writer",
      "tags": [
        "prd",
        "requirements",
        "definition"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Senior Technical Product Manager.\n\n# Context\nWe are building a new feature inside an existing codebase. Use the provided repo context to keep requirements realistic.\n\n# Task\nDraft a PRD for \"{{feature_name}}\" aimed at {{target_users}}.\n\n# Inputs\n- **Feature Name**: {{feature_name}}\n- **Target Users**: {{target_users}}\n- **Success Metrics**: {{success_metrics}}\n- **Context Files**: {{context_files}}\n\n# Requirements\n1. **MVP Scope**: Clearly list what is in and out of scope.\n2. **User Stories**: Use Gherkin (Given/When/Then).\n3. **Functional Requirements**: Include API/UI/workflow requirements that match the existing stack.\n4. **Non-Functional Requirements**: Performance, security, accessibility, and observability.\n5. **Dependencies**: Call out internal/external dependencies.\n6. **Risks & Mitigations**: Identify at least 3 risks.\n7. **Analytics**: Define events or KPIs tied to the success metrics.\n\n# Output Format\nGenerate a `PRD.md` with headings:\n- Overview\n- Goals & Non-Goals\n- User Stories\n- Functional Requirements\n- Non-Functional Requirements\n- UX Notes\n- Data & Analytics\n- Dependencies\n- Risks & Mitigations\n- Open Questions\n",
      "slug": "prd-writer",
      "shortDescription": "Produce a structured PRD with goals, scope, risks, and metrics.",
      "surface": "prototyping",
      "pmStage": "Requirements & Specifications",
      "featured": true,
      "complexity": "Deep",
      "outputType": "Markdown Document",
      "jobsToBeDone": [
        "Draft a PRD from rough notes",
        "Align stakeholders on scope",
        "Document MVP success metrics"
      ],
      "inputs": [
        {
          "key": "feature_name",
          "label": "Feature Name",
          "example": "User Authentication"
        },
        {
          "key": "target_users",
          "label": "Target Users",
          "example": "New and returning end-users who need secure access"
        },
        {
          "key": "success_metrics",
          "label": "Success Metrics",
          "example": "Signup conversion rate, login success rate, support tickets"
        },
        {
          "key": "context_files",
          "label": "Context Files",
          "example": "@package.json @README.md"
        }
      ]
    },
    {
      "id": "react-component",
      "title": "Interactive List Component (React)",
      "category": "prototyping",
      "path": "prototyping/react-component",
      "tags": [
        "react",
        "frontend",
        "component"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a React Developer.\n\n# Context\nWe need a production-ready component that can be dropped into a prototype with minimal wiring.\n\n# Task\nCreate a functional component called `{{component_name}}`.\n\n# Inputs\n- **Data Shape**: {{data_shape}}\n- **Primary Action**: {{primary_action}}\n\n# Requirements\n1. **TypeScript**: Provide `type` definitions for props and data items.\n2. **States**: Handle loading, empty, and populated states.\n3. **Interaction**: Wire the primary action to a button or clickable card.\n4. **Accessibility**: Use semantic HTML and aria labels where needed.\n5. **Styling**: Use simple utility classes or inline styles (no external dependencies).\n\n# Output Format\nReact component code (TSX).\n",
      "slug": "react-component",
      "shortDescription": "Build a reusable React component with empty/loading states.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "featured": true,
      "complexity": "Structured",
      "outputType": "Code (React)",
      "jobsToBeDone": [
        "Build UI components",
        "Prototype frontend features",
        "Handle states cleanly"
      ],
      "inputs": [
        {
          "key": "component_name",
          "label": "Component Name",
          "example": "FeatureList"
        },
        {
          "key": "data_shape",
          "label": "Data Shape",
          "example": "array of features with id, name, description, and optional badge"
        },
        {
          "key": "primary_action",
          "label": "Primary Action",
          "example": "onSelect(featureId)"
        }
      ]
    },
    {
      "id": "slack-bot",
      "title": "Slack Command Bot (Pseudocode)",
      "category": "prototyping",
      "path": "prototyping/slack-bot",
      "tags": [
        "slack",
        "bot",
        "pseudocode"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Software Architect.\n\n# Context\nWe need a Slack slash command that returns a quick system status response for incident triage.\n\n# Task\nOutline pseudocode for a Slack bot that listens for `{{command}}` and checks {{data_source}}.\n\n# Requirements\n1. **Security**: Verify the Slack signature and timestamp.\n2. **Flow**: Acknowledge the request quickly, then fetch status data.\n3. **Response**: Post an ephemeral message with `{{response_message}}` and a short status summary.\n4. **Errors**: Handle timeouts or failures with a friendly error response.\n\n# Output Format\nPseudocode (clear steps, functions, and comments).\n",
      "slug": "slack-bot",
      "shortDescription": "Outline secure Slack command handling with status checks.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Structured",
      "outputType": "Pseudocode",
      "jobsToBeDone": [
        "Design bot logic",
        "Integrate with Slack API",
        "Document request validation"
      ],
      "inputs": [
        {
          "key": "command",
          "label": "Slash Command",
          "example": "/status"
        },
        {
          "key": "data_source",
          "label": "Status Data Source",
          "example": "health check endpoint or database ping"
        },
        {
          "key": "response_message",
          "label": "Success Response",
          "example": "All systems operational"
        }
      ]
    },
    {
      "id": "ui-interaction",
      "title": "Accessible Modal Interaction (HTML/JS)",
      "category": "prototyping",
      "path": "prototyping/ui-interaction",
      "tags": [
        "html",
        "javascript",
        "ui"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a Frontend Developer.\n\n# Context\nWe need a lightweight modal confirmation dialog with good accessibility defaults.\n\n# Task\nProvide an HTML/CSS/JavaScript snippet for a modal dialog.\n\n# Requirements\n1. **Trigger**: User clicks a \"{{trigger_button_text}}\" button.\n2. **Interaction**: A modal appears with the message \"{{modal_message}}\".\n3. **Controls**: Include \"Confirm\" and \"Cancel\" buttons in the modal.\n4. **Accessibility**: Use `role=\"dialog\"`, `aria-modal=\"true\"`, and close on ESC.\n5. **UX**: Clicking the backdrop closes the modal.\n\n# Output Format\nHTML/CSS/JS snippet.\n",
      "slug": "ui-interaction",
      "shortDescription": "Create an accessible confirmation modal with keyboard support.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Quick",
      "outputType": "Code (HTML/JS)",
      "jobsToBeDone": [
        "Prototype UI interactions",
        "Create accessible modals"
      ],
      "inputs": [
        {
          "key": "trigger_button_text",
          "label": "Trigger Button Text",
          "example": "Subscribe"
        },
        {
          "key": "modal_message",
          "label": "Modal Message",
          "example": "Are you sure you want to subscribe?"
        }
      ]
    },
    {
      "id": "unit-test",
      "title": "Unit Test Template (Python)",
      "category": "prototyping",
      "path": "prototyping/unit-test",
      "tags": [
        "python",
        "testing",
        "unittest"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are a QA Engineer or Python Developer.\n\n# Context\nWe need tests that validate boundary conditions and invalid inputs for a core pricing rule.\n\n# Task\nWrite Python unit tests (pytest or unittest) for `{{function_name}}(usage)` based on these rules: {{business_rules}}.\n\n# Requirements\n1. **Coverage**: Test at least one case for each tier.\n2. **Boundaries**: Include tests for exact thresholds and off-by-one values.\n3. **Invalid Input**: Add at least one test for negative or non-numeric input.\n4. **Clarity**: Use descriptive test names and comments for each scenario.\n\n# Output Format\nPython test code.\n",
      "slug": "unit-test",
      "shortDescription": "Write robust unit tests with edge cases and boundaries.",
      "surface": "prototyping",
      "pmStage": "Risk, Quality & Validation",
      "complexity": "Structured",
      "outputType": "Code (Python)",
      "jobsToBeDone": [
        "Ensure code quality",
        "Write test cases",
        "Capture edge conditions"
      ],
      "inputs": [
        {
          "key": "function_name",
          "label": "Function Name",
          "example": "calculate_tier"
        },
        {
          "key": "business_rules",
          "label": "Business Rules",
          "example": "Free < 100, Pro 100-999, Enterprise >= 1000"
        }
      ]
    },
    {
      "id": "website-mockup",
      "title": "Landing Page Mockup (HTML/CSS)",
      "category": "prototyping",
      "path": "prototyping/website-mockup",
      "tags": [
        "html",
        "css",
        "prototype"
      ],
      "updatedAt": "2026-01-27T22:03:51+02:00",
      "prompt": "# Role\nYou are an Expert Frontend Developer.\n\n# Context\nWe need a quick landing page to validate messaging and layout for a {{product_type}}.\n\n# Task\nGenerate HTML and CSS for the landing page.\n\n# Requirements\n1. **Header**: Logo placeholder and navigation links.\n2. **Hero**: Headline, subheadline, and a primary CTA button labeled \"{{primary_cta}}\".\n3. **Social Proof**: Include a short testimonials section.\n4. **Features**: Highlight three key features with icon placeholders.\n5. **Pricing Preview**: Simple two-tier pricing cards.\n6. **Style**: Clean, modern, responsive, and accessible.\n\n# Output Format\nSingle HTML file with embedded CSS.\n",
      "slug": "website-mockup",
      "shortDescription": "Generate a responsive landing page with product messaging.",
      "surface": "prototyping",
      "pmStage": "Product & UX Prototypes",
      "complexity": "Structured",
      "outputType": "Code (HTML/CSS)",
      "jobsToBeDone": [
        "Create quick landing page",
        "Visualize web layout",
        "Showcase product value"
      ],
      "inputs": [
        {
          "key": "product_name",
          "label": "Product Name",
          "example": "CRMify"
        },
        {
          "key": "product_type",
          "label": "Product Type",
          "example": "SaaS CRM"
        },
        {
          "key": "primary_cta",
          "label": "Primary Call-to-Action",
          "example": "Start Free Trial"
        }
      ]
    }
  ]
}